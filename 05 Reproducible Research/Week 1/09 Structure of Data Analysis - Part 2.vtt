WEBVTT

1
00:00:00.790 --> 00:00:02.140
In this lecture, we're going to continue
the

2
00:00:02.140 --> 00:00:05.200
data analysis example that we started in
part one.

3
00:00:05.200 --> 00:00:07.570
If you recall, we, we laid down, kind of a
list of

4
00:00:07.570 --> 00:00:12.160
te, of steps that generally one might take
when doing a data analysis.

5
00:00:12.160 --> 00:00:16.410
And previously we talked about the first
roughly half of these steps.

6
00:00:16.410 --> 00:00:18.680
And in this lecture, we're going to talk
about the remaining half.

7
00:00:18.680 --> 00:00:22.200
So this includes exploratory data
analysis, statistical

8
00:00:22.200 --> 00:00:26.470
prediction and modeling, interpretation,
challenging your results, synthesizing

9
00:00:26.470 --> 00:00:28.930
and writing up the results, and creating
reproducible code.

10
00:00:30.540 --> 00:00:32.880
So if you recall, the basic question was,
can

11
00:00:32.880 --> 00:00:36.570
I automatically detect emails that are
SPAM or not?

12
00:00:36.570 --> 00:00:39.870
And a more slightly, more concrete version
of this

13
00:00:39.870 --> 00:00:42.070
question that can be used to translate
into a Cisco

14
00:00:42.070 --> 00:00:45.570
problem was, you know, can I use
quantitative characteristics

15
00:00:45.570 --> 00:00:48.300
of the emails to classify them as SPAM or
HAM?

16
00:00:50.590 --> 00:00:53.670
So, our data set, again, was, from

17
00:00:53.670 --> 00:00:57.580
this, UCI machine, Learning Repository,
which had already

18
00:00:57.580 --> 00:01:02.910
been cleaned up, and it was available in
the, current lab package as a data set.

19
00:01:02.910 --> 00:01:07.500
So this data set had 4,600, observations,
or emails,

20
00:01:07.500 --> 00:01:10.420
that had been kind of characterized along
58 different variables.

21
00:01:12.500 --> 00:01:14.900
so, the first thing that we need to do
with this data set if

22
00:01:14.900 --> 00:01:19.980
we want to build a model to kind of,
classify emails into spam or not.

23
00:01:19.980 --> 00:01:24.030
Is that we need to split the data set into
test set and a training set.

24
00:01:24.030 --> 00:01:26.080
So the idea is that we're going to use
part of the test

25
00:01:26.080 --> 00:01:29.650
of the data set to build our model, and
then we're going to

26
00:01:29.650 --> 00:01:32.360
use another part of the data set which is
independent of the first

27
00:01:32.360 --> 00:01:36.850
part to actually determine how good our
model is kind of making a prediction.

28
00:01:36.850 --> 00:01:38.360
So here I'm

29
00:01:38.360 --> 00:01:41.590
a taking a random half of the data set, so

30
00:01:41.590 --> 00:01:44.790
I'm using, I'm flipping a coin with the
rbinom function, to

31
00:01:44.790 --> 00:01:48.280
generate a random kind of coin flip with
probability of

32
00:01:48.280 --> 00:01:52.030
half so that'll separate the the data set
into two pieces.

33
00:01:52.030 --> 00:01:54.910
So you can see that roughly 2000, so 2314
are going

34
00:01:54.910 --> 00:01:59.160
to be one half and 2287 will be in the
other half.

35
00:01:59.160 --> 00:02:03.380
And so the training set will be, will be,
one set and

36
00:02:03.380 --> 00:02:06.090
the test set will be another set of data.

37
00:02:08.280 --> 00:02:09.310
So the first thing we're going to want to

38
00:02:09.310 --> 00:02:11.430
do is a little bit of exploratory data
analysis.

39
00:02:11.430 --> 00:02:13.530
We have not looked at this data set yet.

40
00:02:13.530 --> 00:02:15.440
And so it would be useful to look at kind

41
00:02:15.440 --> 00:02:17.662
of what are the, what data, what did the
data

42
00:02:17.662 --> 00:02:20.882
look like, what's the distribution of the
data, you know

43
00:02:20.882 --> 00:02:24.680
what what are the relationships between
the variables, things like that.

44
00:02:24.680 --> 00:02:26.570
So we want to look at basic summaries one

45
00:02:26.570 --> 00:02:29.440
dimensional, two dimensional summaries of
the data we want to

46
00:02:29.440 --> 00:02:31.220
check for is there are any missing data,
you

47
00:02:31.220 --> 00:02:33.940
know why is there missing data, if there
is create

48
00:02:33.940 --> 00:02:38.220
some exploratory plots and do a little
kind of exploratory analyses.

49
00:02:38.220 --> 00:02:40.600
So so if we look at the training data
sets,

50
00:02:40.600 --> 00:02:42.300
so that's what we're going to focus on
right now as

51
00:02:42.300 --> 00:02:45.120
we do our exploratory analysis, as we
build our model,

52
00:02:45.120 --> 00:02:47.440
all that's going to be done in the
training data set.

53
00:02:47.440 --> 00:02:51.290
And if you look at the, the column names
of the dataset, you can see that they're

54
00:02:51.290 --> 00:02:55.870
all just words essentially and and if you
look

55
00:02:55.870 --> 00:03:00.350
at the first five rows, we can see that

56
00:03:00.350 --> 00:03:06.380
basically that these are the frequencies
at which they occur in a given email.

57
00:03:06.380 --> 00:03:09.970
So you can see, you can see the work make
does not appear in

58
00:03:09.970 --> 00:03:14.090
that first email and, and the word mail
does not appear, so things like that.

59
00:03:14.090 --> 00:03:16.520
So these are all basically frequency
counts, or

60
00:03:16.520 --> 00:03:19.920
frequencies of, of words within each of
the emails.

61
00:03:21.310 --> 00:03:25.920
So if we look at the training data set,
and look at the outcome

62
00:03:25.920 --> 00:03:30.620
we see that 906 of, of the emails are
spam, are classified as spam.

63
00:03:30.620 --> 00:03:33.420
And the other 1381 are classified as
non-spam.

64
00:03:33.420 --> 00:03:34.890
So these, this is what we're going to use
to

65
00:03:34.890 --> 00:03:38.150
kind of build our model for predicting the
spam emails.

66
00:03:39.330 --> 00:03:41.950
We can make some plots and we can compare,
you know, so what are

67
00:03:41.950 --> 00:03:47.190
the frequen, the frequencies of certain
characteristics

68
00:03:47.190 --> 00:03:48.930
between the spam and the non spam emails.

69
00:03:48.930 --> 00:03:51.800
So, here we're looking at a variable
called capital ave.

70
00:03:51.800 --> 00:03:54.030
So the average number of capital letters.

71
00:03:54.030 --> 00:03:56.560
And, you can see that its difficult to
look

72
00:03:56.560 --> 00:03:58.730
at this picture, because the data are
highly skewed.

73
00:03:58.730 --> 00:04:02.160
And so, in these kinds of situations it's
often useful to

74
00:04:02.160 --> 00:04:05.306
just kind of look at the log
transformation of the variable.

75
00:04:05.306 --> 00:04:08.870
So, here I'm going to to take the base ten
log of the data

76
00:04:08.870 --> 00:04:12.410
set, or, I'm sorry, the variable, and
compare them to spam and nonspam.

77
00:04:12.410 --> 00:04:14.500
And since there are a lot of zeros in

78
00:04:14.500 --> 00:04:16.870
this particular variable, taking the log
of zero doesn't

79
00:04:16.870 --> 00:04:17.930
really make sense.

80
00:04:17.930 --> 00:04:20.730
So we'll just add 1 to that variable, just
so we can take the

81
00:04:20.730 --> 00:04:24.430
log and kind of get a rough sense of what
the data look like.

82
00:04:24.430 --> 00:04:29.010
Typically, you don't, you wouldn't want to
just add 1 to a variable just because.

83
00:04:29.010 --> 00:04:31.930
But since we're just exploring the data,
a, like, making, kind

84
00:04:31.930 --> 00:04:34.240
of, exploratory plots, it's okay to do
that in this case.

85
00:04:34.240 --> 00:04:39.250
So here you can see, rather obviously,
that, the

86
00:04:39.250 --> 00:04:42.240
spam emails have a much higher rate of
these,

87
00:04:42.240 --> 00:04:45.320
capital letters, than the non spam emails,
and of course, if

88
00:04:45.320 --> 00:04:49.060
you've ever seen spam emails, you're
probably familiar with that phenomenon.

89
00:04:49.060 --> 00:04:51.550
And so that's one useful, relationship to
see there.

90
00:04:53.080 --> 00:04:55.380
We can look at pairwise relationships

91
00:04:55.380 --> 00:04:58.230
between the different variables in the
plots.

92
00:04:58.230 --> 00:05:01.330
And here I, I've got a pairs plot of a few
of the

93
00:05:01.330 --> 00:05:04.870
variables, and as this is the log
transformation of each of the variables.

94
00:05:04.870 --> 00:05:07.320
And you can see that some of them are
correlated,

95
00:05:07.320 --> 00:05:10.810
some of them are not particularly
correlated, and so that's useful to know.

96
00:05:12.970 --> 00:05:14.580
So we can explore the predictors space a

97
00:05:14.580 --> 00:05:17.940
little bit more by doing a hierarchical
cluster analysis

98
00:05:17.940 --> 00:05:22.320
and so this is a first cut at trying to do
that with the hclust function in R.

99
00:05:22.320 --> 00:05:25.330
And you can see I plotted the Dendrogram
just to, to see kind

100
00:05:25.330 --> 00:05:26.680
of how what, what predictors or what

101
00:05:26.680 --> 00:05:29.840
words or characteristics tend to cluster
together.

102
00:05:29.840 --> 00:05:32.460
And it's not particularly helpful at this
point although

103
00:05:32.460 --> 00:05:35.710
it does separate out this one variable
capital total.

104
00:05:35.710 --> 00:05:37.980
But if you recall that the clustering
algorithms can

105
00:05:37.980 --> 00:05:42.080
be sensitive to any skewness in the
distribution of the individual variables.

106
00:05:42.080 --> 00:05:44.340
So it may be useful to redo the

107
00:05:44.340 --> 00:05:48.260
clustering analysis after a transformation
of the predictor space.

108
00:05:49.890 --> 00:05:52.120
So here I've taken a log, a base 10 log

109
00:05:52.120 --> 00:05:55.650
transformation of the fifth, of the
predictors in the training

110
00:05:55.650 --> 00:05:57.930
data set, and again, I've added one to
each one,

111
00:05:57.930 --> 00:06:00.950
just so, to make, to avoid taking the log
of zero.

112
00:06:00.950 --> 00:06:03.535
And now you can see it's a little bit more
interesting,

113
00:06:03.535 --> 00:06:07.055
the dendrogram that is, it's separated out
a few clusters wi-,

114
00:06:07.055 --> 00:06:10.460
this capital average is one kind of
cluster all by itself.

115
00:06:10.460 --> 00:06:14.350
There's another cluster that cludes, that
includes you will or your.

116
00:06:14.350 --> 00:06:15.280
And then there are a bunch of other

117
00:06:15.280 --> 00:06:18.830
words that kind of lump more ambiguously
together.

118
00:06:18.830 --> 00:06:21.420
And so this may be something worth
exploring a little bit

119
00:06:21.420 --> 00:06:24.880
further if you see some particular kind of
characteristics that are interesting.

120
00:06:27.330 --> 00:06:29.730
So once we've done exploratory data
analysis, we've looked

121
00:06:29.730 --> 00:06:31.940
at some univariate and bivariate plots, we
did a

122
00:06:31.940 --> 00:06:36.360
little cluster analysis, we we can move on
to

123
00:06:36.360 --> 00:06:40.700
doing a more sophisticated statistical
model and some prediction modeling.

124
00:06:40.700 --> 00:06:45.070
And so any statistical modeling that you
engage in should be informed by you know,

125
00:06:45.070 --> 00:06:46.490
kind of question that you're interested
in, of

126
00:06:46.490 --> 00:06:49.570
course, and the results of any exploratory
analysis.

127
00:06:49.570 --> 00:06:52.600
The exact methods that you employ will
depend

128
00:06:52.600 --> 00:06:53.840
on, you know, the question of interest.

129
00:06:55.186 --> 00:06:58.480
And when you do a statistical model, you
should account for the fact that

130
00:06:58.480 --> 00:07:00.910
the data have been processed or
transformed

131
00:07:00.910 --> 00:07:03.440
if they have, in fact, been so.

132
00:07:03.440 --> 00:07:05.530
And when you, as you do statistical

133
00:07:05.530 --> 00:07:06.800
modeling, you should always think about,
what

134
00:07:06.800 --> 00:07:08.530
are the measures of uncertainty, what are

135
00:07:08.530 --> 00:07:11.380
the sources of uncertainty in your data
set.

136
00:07:13.350 --> 00:07:16.470
So here we're going to just do a very
basic statistical model.

137
00:07:16.470 --> 00:07:18.060
What we're going to do is we're going

138
00:07:18.060 --> 00:07:22.380
to go through each of the variables in the
data

139
00:07:22.380 --> 00:07:25.150
set and try to fit a generalizing model,
in this case

140
00:07:25.150 --> 00:07:28.740
a logistic regression, to see if we can
predict an

141
00:07:28.740 --> 00:07:31.270
email is spam or not by using just a
single variable.

142
00:07:31.270 --> 00:07:35.230
So here using the reformulate function to
create a formula that

143
00:07:35.230 --> 00:07:38.900
includes the response, which is just the
type, type of email.

144
00:07:38.900 --> 00:07:43.270
And one of the variables of the data set,
and we're just going to cycle through

145
00:07:43.270 --> 00:07:45.410
all the variables in this data set using

146
00:07:45.410 --> 00:07:49.340
this for-loop to build a logistic
regression model.

147
00:07:49.340 --> 00:07:52.250
and, and then subsequently calculate the
cross validated error

148
00:07:52.250 --> 00:07:56.520
rate of predicting spam emails from a
single, variable.

149
00:07:56.520 --> 00:07:59.264
And so, if you run this loop in R, it may
take a little bit to

150
00:07:59.264 --> 00:08:00.776
run, it won't, but if it has to

151
00:08:00.776 --> 00:08:03.770
loop through all the variables,
[INAUDIBLE] all the models.

152
00:08:03.770 --> 00:08:05.090
So, once we've done this, we're going to
try

153
00:08:05.090 --> 00:08:08.520
to figure out, well, which of the
individual variables,

154
00:08:08.520 --> 00:08:10.680
has the minimum cross validated error
rate.

155
00:08:10.680 --> 00:08:12.990
And so we can just go, and you can take
this vector of

156
00:08:12.990 --> 00:08:17.050
results this CV error, and just figure out
which one is the minimum.

157
00:08:17.050 --> 00:08:20.970
And it turns out that the, the predictor
that has the

158
00:08:20.970 --> 00:08:25.330
minimum cross validated error rate is this
variable called char dollar.

159
00:08:25.330 --> 00:08:28.430
This is an indicator of the number of
dollar signs in the email.

160
00:08:29.440 --> 00:08:31.800
So, just keep in mind this is a very
simple model.

161
00:08:31.800 --> 00:08:33.830
Each of these models that we fit only have
a single

162
00:08:33.830 --> 00:08:34.910
predictor in it.

163
00:08:34.910 --> 00:08:36.670
So of course we could maybe think of
something

164
00:08:36.670 --> 00:08:39.780
more complicated, but this maybe an
interesting place to start.

165
00:08:42.080 --> 00:08:46.420
So, if we take this best model from this
set of 55 predictors,

166
00:08:46.420 --> 00:08:51.120
this, this char dollar variable and I'll
just re-fit the model again right here.

167
00:08:51.120 --> 00:08:53.710
And so this is a logistic regression
model.

168
00:08:53.710 --> 00:08:59.680
We can actually make predictions now from
the model on the test data recall that we

169
00:08:59.680 --> 00:09:01.190
split the data set into two parts and

170
00:09:01.190 --> 00:09:03.085
built the training model on the training
data set.

171
00:09:03.085 --> 00:09:05.150
And so now we're going to predict the
outcome on

172
00:09:05.150 --> 00:09:07.690
the test data set to see how well we do.

173
00:09:07.690 --> 00:09:11.820
And so, in a logistic regression we don't
get

174
00:09:11.820 --> 00:09:14.396
we don't get specific predictions out of
you know 0

175
00:09:14.396 --> 00:09:17.540
1 classifications of each of the messages
we get a

176
00:09:17.540 --> 00:09:20.510
probability that a message is going to be
spam or not.

177
00:09:20.510 --> 00:09:21.460
And so then we have to take this

178
00:09:21.460 --> 00:09:25.120
continuous probability, which ranges
between 0 and 1,

179
00:09:25.120 --> 00:09:26.800
and, and determine at what point, at what

180
00:09:26.800 --> 00:09:30.150
cutoff, do we think that the email is
spam.

181
00:09:30.150 --> 00:09:32.750
And so we're, we're just going to draw the
cut off here at 0.5,

182
00:09:32.750 --> 00:09:39.830
so if the probability is above 50%, we're
just going to call it a spam email.

183
00:09:43.210 --> 00:09:45.870
So once we've created our classification,
we can take a

184
00:09:45.870 --> 00:09:50.090
look at the predicted values for, from our
model, and then

185
00:09:50.090 --> 00:09:52.890
compare them with the actual values from
the test data set,

186
00:09:52.890 --> 00:09:56.030
because we know what, which was spam, and
which was not.

187
00:09:56.030 --> 00:09:58.860
And here's the classification table that
we get

188
00:09:58.860 --> 00:10:03.310
from the predicted and the the real
values.

189
00:10:03.310 --> 00:10:05.240
And we can, so we can just calculate the
error rate.

190
00:10:05.240 --> 00:10:08.510
And so the, the mistakes that we made are
on the off diagonal

191
00:10:08.510 --> 00:10:14.950
elements of this table, so 61 and 458.
So, 61 were classified as spam, that were

192
00:10:14.950 --> 00:10:20.560
not actually spam, and 458 were classify
as non spam but actually were spam.

193
00:10:20.560 --> 00:10:24.840
So we calculate this error rate as about
22%.

194
00:10:24.840 --> 00:10:29.750
So, now that we've done the analysis,
we've calculated some results.

195
00:10:29.750 --> 00:10:32.050
We've calculated our kind of our best
model.

196
00:10:32.050 --> 00:10:33.700
We've looked at the error rate that's
produced

197
00:10:33.700 --> 00:10:34.360
by that model.

198
00:10:35.430 --> 00:10:37.500
So now we need to interpret our findings
and it's

199
00:10:37.500 --> 00:10:41.230
important when you interpret your findings
to use appropriate language.

200
00:10:41.230 --> 00:10:43.890
And to not be to not use language

201
00:10:43.890 --> 00:10:46.640
that goes beyond the analysis that you
actually did.

202
00:10:46.640 --> 00:10:48.580
And so you want to give kind of, if you're
in this type

203
00:10:48.580 --> 00:10:50.110
of application where we're just looking at

204
00:10:50.110 --> 00:10:52.280
some data, we're building a predictive
model.

205
00:10:52.280 --> 00:10:56.170
You want to use works like, you know,
prediction or it correlates with

206
00:10:56.170 --> 00:10:59.710
or, or, or certain variables may be
associated with the outcome or

207
00:10:59.710 --> 00:11:04.080
the analysis is descriptive, and so and so
just to think

208
00:11:04.080 --> 00:11:08.800
about carefully what kind of language you
use to interpret your results.

209
00:11:08.800 --> 00:11:10.910
it's, it's good to give an explanation, so
if

210
00:11:10.910 --> 00:11:14.700
you can think of, you know, why certain
models predict

211
00:11:14.700 --> 00:11:16.060
better than others, it would be useful to
kind

212
00:11:16.060 --> 00:11:18.780
of give an explanation of what you think
that is.

213
00:11:18.780 --> 00:11:20.430
If there are coefficients in the model
that you

214
00:11:20.430 --> 00:11:23.200
need to interpret it's useful, you can do
that here.

215
00:11:23.200 --> 00:11:24.800
And in particular it's useful to

216
00:11:24.800 --> 00:11:26.920
bring in measures of uncertainty, to kind

217
00:11:26.920 --> 00:11:30.670
of calibrate your interpretation of the
final results.

218
00:11:32.360 --> 00:11:34.650
So, in this example, we might think, you
know,

219
00:11:34.650 --> 00:11:36.940
that you might think of, of stating that,
you

220
00:11:36.940 --> 00:11:39.750
know, the fraction of characters that are
dollar signs,

221
00:11:39.750 --> 00:11:41.800
can be used to predict if an email spam.

222
00:11:42.840 --> 00:11:46.140
Maybe we decide that anything more, with
more

223
00:11:46.140 --> 00:11:48.750
than 6.6% dollar signs is classified as
spam.

224
00:11:48.750 --> 00:11:50.130
More dollar signs always

225
00:11:50.130 --> 00:11:52.870
means more spam under our prediction
model.

226
00:11:52.870 --> 00:12:00.060
And, and in our for our model in the test
data set, the error rate was 22.4%.

227
00:12:00.060 --> 00:12:04.650
So, once you've done your analysis and
you've developed your interpretation,

228
00:12:04.650 --> 00:12:06.940
it's important that you, yourself,
challenge

229
00:12:06.940 --> 00:12:09.190
all the results that you've found.

230
00:12:09.190 --> 00:12:12.370
Because if you don't do it, someone else
is going to do it once they see your

231
00:12:12.370 --> 00:12:13.900
analysis, and so you might as well get one

232
00:12:13.900 --> 00:12:16.550
step ahead of everyone by doing it
yourself first.

233
00:12:16.550 --> 00:12:18.360
And so it's good to challenge everything,
every, the

234
00:12:18.360 --> 00:12:22.160
whole process by which you gone through
this problem.

235
00:12:22.160 --> 00:12:24.580
The question itself is that, is the
question even a

236
00:12:24.580 --> 00:12:28.020
valid question to ask where the data came
from, how

237
00:12:28.020 --> 00:12:30.840
you got the data, how you processed the
data, how

238
00:12:30.840 --> 00:12:33.940
you did the analysis and any conclusions
that you drew.

239
00:12:35.290 --> 00:12:36.590
If you have measures of uncertainty,

240
00:12:36.590 --> 00:12:38.600
are those the appropriate measure of
uncertainty.

241
00:12:39.770 --> 00:12:41.560
And if you built models, you know

242
00:12:41.560 --> 00:12:43.490
why is your model the best model?

243
00:12:43.490 --> 00:12:45.540
Why is it an appropriate model for this
problem?

244
00:12:46.920 --> 00:12:49.170
How do you choose the things to include in
your model?

245
00:12:49.170 --> 00:12:51.760
So all these things are questions that you
should ask yourself and should have a

246
00:12:51.760 --> 00:12:56.520
reasonable answer to so that when someone
else asks you, you can respond in kind.

247
00:12:57.590 --> 00:13:00.360
And also it's useful to think potential
alternative analyses

248
00:13:00.360 --> 00:13:02.850
that, you know, might be useful it doesn't
mean

249
00:13:02.850 --> 00:13:05.070
that you have to do those alternative
analyses, in

250
00:13:05.070 --> 00:13:06.730
the sense that you might stick to your
original

251
00:13:06.730 --> 00:13:09.220
just because other reasons.

252
00:13:09.220 --> 00:13:11.850
But it may be useful to try alternative
analyses just in case

253
00:13:11.850 --> 00:13:16.660
they may be useful in different ways or
may produce better predictions.

254
00:13:20.230 --> 00:13:22.300
Once you've interpreted your results,
you've done the

255
00:13:22.300 --> 00:13:24.610
analysis, you've interpreted your results,
you've drawn some

256
00:13:24.610 --> 00:13:27.530
conclusions, you've challenged all your
findings you're going to

257
00:13:27.530 --> 00:13:30.280
need to synthesize the results and write
them up.

258
00:13:30.280 --> 00:13:34.100
So synthesis is very important because
typically in any data analysis,

259
00:13:34.100 --> 00:13:36.830
there are going to be many, many, many
things that you did.

260
00:13:36.830 --> 00:13:40.730
And when you present them to a, another
person or to a group you're going to

261
00:13:40.730 --> 00:13:45.130
want to have to winnow it down to the kind
of most important aspects and to, to,

262
00:13:45.130 --> 00:13:46.970
to tell a coherent story.

263
00:13:46.970 --> 00:13:49.900
And so typically you want to lead with the
question that you were trying to address.

264
00:13:49.900 --> 00:13:52.620
If people understand the question then
they can

265
00:13:52.620 --> 00:13:54.320
make, they can draw up a context in

266
00:13:54.320 --> 00:13:56.380
their mind, and understand, kind of have a

267
00:13:56.380 --> 00:13:59.890
better understanding of the framework in
which you're operating.

268
00:13:59.890 --> 00:14:02.840
And so that will lead to what kinds of
data are necessary, are

269
00:14:03.960 --> 00:14:05.970
are, are appropriate for this question

270
00:14:05.970 --> 00:14:08.810
what kinds of analyses would be
appropriate.

271
00:14:08.810 --> 00:14:10.900
So you can summarize the analyses

272
00:14:10.900 --> 00:14:12.440
as you're telling the story.

273
00:14:12.440 --> 00:14:15.580
It's important that you don't include
every analysis that you ever did

274
00:14:15.580 --> 00:14:18.140
but only if its needed for kind of telling
a coherent story.

275
00:14:19.575 --> 00:14:23.560
It's useful to sometimes keep these
analyses in your back pocket though, even

276
00:14:23.560 --> 00:14:26.610
if you don't talk about it, because
someone may challenge what you've done

277
00:14:26.610 --> 00:14:28.940
and it's useful to say well you know we
did do that analysis

278
00:14:28.940 --> 00:14:32.180
but, it was problematic you know because
of whatever the reason may be.

279
00:14:34.150 --> 00:14:35.930
It's important to order the analysis

280
00:14:35.930 --> 00:14:39.600
that you did according to the story that
you're telling and often that order

281
00:14:39.600 --> 00:14:43.240
is not the same as the order in which you
actually did the analysis.

282
00:14:43.240 --> 00:14:46.950
So, it's usually not that useful to talk
about the analysis that

283
00:14:46.950 --> 00:14:49.690
you did kind of chronologically, or the
order in which you did

284
00:14:49.690 --> 00:14:51.870
them, because the order in which you did
them is often very

285
00:14:51.870 --> 00:14:58.190
scattered and and and not, and kind of
doesn't make sense in retrospect.

286
00:14:58.190 --> 00:15:01.130
So talk about the analyses in your, of
your data set in the order that,

287
00:15:01.130 --> 00:15:03.080
that's appropriate for the story you're
trying to tell.

288
00:15:04.150 --> 00:15:06.620
And when your telling the story or you're
presenting to

289
00:15:06.620 --> 00:15:09.040
someone or to your group it's, it's useful
to include kind

290
00:15:09.040 --> 00:15:12.800
of very well done figures so that people
can kind of

291
00:15:12.800 --> 00:15:15.380
understand what you're trying to say in
one picture or two.

292
00:15:17.490 --> 00:15:20.740
So, in our example, the basic question was
you know, can we

293
00:15:20.740 --> 00:15:25.240
use quantitative characteristics of the
emails to classify them as spam or ham.

294
00:15:25.240 --> 00:15:27.350
Our approach was you know rather than try
to

295
00:15:27.350 --> 00:15:30.530
get the ideal data set from all Google
servers as

296
00:15:30.530 --> 00:15:33.930
we collected some data from the UCI
machine learning repository

297
00:15:33.930 --> 00:15:37.050
and created training and test sets from
this data set.

298
00:15:37.050 --> 00:15:41.130
We explored some relationships between the
various predictors.

299
00:15:41.130 --> 00:15:42.890
We decided to use a logistic regression

300
00:15:42.890 --> 00:15:47.840
model on the training set and chose our
kind of single variable predictor by using

301
00:15:47.840 --> 00:15:50.830
cross validation when we applied this
model to

302
00:15:50.830 --> 00:15:53.406
the test set it was 78% ac-, accurate.

303
00:15:54.490 --> 00:15:57.090
So it, the interpretation of our results

304
00:15:57.090 --> 00:15:59.492
was that basically, more dollar signs
seemed

305
00:15:59.492 --> 00:16:06.530
to indicate an email was more likely to be
a spam, and this seems reasonable.

306
00:16:06.530 --> 00:16:07.850
We've all seen emails with you know,

307
00:16:07.850 --> 00:16:10.480
lots of dollar signs in them trying to
sell you something.

308
00:16:10.480 --> 00:16:14.370
And so this is kind of both reasonable and
understandable.

309
00:16:15.540 --> 00:16:18.540
Of course, the results were not
particularly great as 78% test

310
00:16:18.540 --> 00:16:22.530
set accuracy is not that good for most
prediction types of algorithms.

311
00:16:22.530 --> 00:16:24.040
That we probably could do much better if
we

312
00:16:24.040 --> 00:16:27.960
included more variables or if we did or we
included

313
00:16:27.960 --> 00:16:30.600
a more sophisticated model, maybe a
non-linear model and

314
00:16:30.600 --> 00:16:33.200
for example is not, why did we use
logistic regression?

315
00:16:33.200 --> 00:16:36.760
We could have used a much more
sophisticated type of modeling approach.

316
00:16:36.760 --> 00:16:39.930
But anyway these are the kinds of things
that you want to outline to

317
00:16:39.930 --> 00:16:43.750
people as you go through data analysis,
and you present it to other people.

318
00:16:43.750 --> 00:16:45.800
So finally, the, the thing that you want
to make sure of

319
00:16:45.800 --> 00:16:50.530
is that you, is that you document your
analysis as you go.

320
00:16:50.530 --> 00:16:53.540
You can use things like tools like R
Markdown and Knitter and

321
00:16:53.540 --> 00:16:56.640
R Studio to kind of document your analyses
as you do them.

322
00:16:56.640 --> 00:16:58.390
And so you can preserve the R code

323
00:16:58.390 --> 00:17:01.550
as well as any kind of a written summary

324
00:17:01.550 --> 00:17:05.200
of your analysis in a single document
using Knitter.

325
00:17:05.200 --> 00:17:06.880
And so then, and so to make sure that

326
00:17:06.880 --> 00:17:10.220
all of what you do is reproducible by
either yourself

327
00:17:10.220 --> 00:17:13.260
or by other people because ultimately
that's the standard by

328
00:17:13.260 --> 00:17:17.520
which most kind of, big data analysis will
be judged.

329
00:17:17.520 --> 00:17:20.832
If someone can not reproduce it then the
conclusions that

330
00:17:20.832 --> 00:17:23.592
you draw will be, will be you know not as
worth,

331
00:17:23.592 --> 00:17:27.740
as worthy as one analysis where the
results are reproducible.

332
00:17:27.740 --> 00:17:29.180
So try to stay organized.

333
00:17:29.180 --> 00:17:30.764
Try to kind of, use the tools

334
00:17:30.764 --> 00:17:34.892
reproducible research to keep things
organized and reproducible.

335
00:17:34.892 --> 00:17:36.835
And and so ,and that will make

336
00:17:36.835 --> 00:17:40.080
your evidence for your conclusions much
more powerful.