WEBVTT

1
00:00:00.560 --> 00:00:02.060
This lecture will talk of, about the

2
00:00:02.060 --> 00:00:05.360
basic process by which data analysis will
unfold.

3
00:00:05.360 --> 00:00:07.660
And of course, not every data analysis is
the same,

4
00:00:07.660 --> 00:00:11.150
and not every data analysis will require
the same components.

5
00:00:11.150 --> 00:00:14.460
I think this will serve as a useful
template for kind of understanding, what

6
00:00:14.460 --> 00:00:18.260
are the pieces of a, of a data analysis
and how they typically flow together.

7
00:00:20.520 --> 00:00:23.060
So, if one were to write down the steps in
a

8
00:00:23.060 --> 00:00:26.370
data analysis, you might come up with
something along these lines.

9
00:00:26.370 --> 00:00:27.390
A list just like this one.

10
00:00:27.390 --> 00:00:30.162
There may be things that, little things
you might want to add or delete.

11
00:00:30.162 --> 00:00:35.420
But see, most data analysis have some
subset of these steps.

12
00:00:35.420 --> 00:00:38.540
And so what we're going to talk about in
this lecture, which is part one.

13
00:00:38.540 --> 00:00:41.330
Is defining the question that you're
interested in.

14
00:00:41.330 --> 00:00:43.270
Defining the ideal data set.

15
00:00:43.270 --> 00:00:46.440
Determining what data you actually can
access.

16
00:00:46.440 --> 00:00:48.190
Obtaining the data and cleaning the data.

17
00:00:48.190 --> 00:00:49.740
And then in part two of this

18
00:00:49.740 --> 00:00:51.990
lecture, we'll talk about the remaining
topics here.

19
00:00:53.960 --> 00:00:57.425
I think the key challenge in, in pretty
much any data analysis was well

20
00:00:57.425 --> 00:00:59.680
characterized by Dan Meyer who's a
mathematics

21
00:00:59.680 --> 00:01:02.864
educator and he taught high school
mathematics.

22
00:01:02.864 --> 00:01:06.219
In his Ted talk he said ask yourselves
what problem have you solved

23
00:01:06.219 --> 00:01:11.150
ever, that was worth solving where you
knew all the given information in advance.

24
00:01:11.150 --> 00:01:11.510
Where you

25
00:01:11.510 --> 00:01:14.290
didn't have a surplus of information and
have to filter it out.

26
00:01:14.290 --> 00:01:17.685
Or you had insuf, insufficient information
and had to go find some.

27
00:01:17.685 --> 00:01:21.906
And so I think that's a key element of
data analysis that which is that you know,

28
00:01:21.906 --> 00:01:24.237
typically, you don't have all the facts or

29
00:01:24.237 --> 00:01:26.694
you have too much information, and you
kind of

30
00:01:26.694 --> 00:01:30.852
have to go through it, and the process, a
lot of the process of data analysis is

31
00:01:30.852 --> 00:01:33.689
sorting through kind of all this, all this
stuff

32
00:01:33.689 --> 00:01:36.838
And so, the first part, the, the kind of
important

33
00:01:36.838 --> 00:01:41.241
part of data analysis that you want to
start with is, is define a question.

34
00:01:41.241 --> 00:01:46.150
And not every data analysis starts with
the very specific or coherent question.

35
00:01:46.150 --> 00:01:49.860
But the, the more effort you can put into
coming up with a reasonable

36
00:01:49.860 --> 00:01:53.120
question, the, the less effort you'll
spend

37
00:01:53.120 --> 00:01:55.820
having to filter through a lot of, stuff.

38
00:01:55.820 --> 00:02:00.180
And the reason why is that defining a
question is the kind of the most powerful.

39
00:02:00.180 --> 00:02:01.860
Dimension reduction tool you can

40
00:02:01.860 --> 00:02:03.445
ever employ.

41
00:02:03.445 --> 00:02:05.630
because if you're interested in, you know,
in,

42
00:02:05.630 --> 00:02:09.160
in, a specific variable, like height or
weight, then

43
00:02:09.160 --> 00:02:11.130
you can kind of remove, a lot of other

44
00:02:11.130 --> 00:02:14.020
variables that don't really pertain to
that at all.

45
00:02:14.020 --> 00:02:15.410
But if you're interested in a different
type

46
00:02:15.410 --> 00:02:17.350
of variable then you can remove another
subset.

47
00:02:17.350 --> 00:02:18.690
And so the idea is if, if you

48
00:02:18.690 --> 00:02:22.920
can narrow down your question as
specifically as possible.

49
00:02:22.920 --> 00:02:27.050
Then that will serve to reduce the kind of
noise that you'll, that you'll have

50
00:02:27.050 --> 00:02:31.200
to deal with when you're going through a
potentially very large data set.

51
00:02:31.200 --> 00:02:33.050
Now sometimes you just want to look at a
data

52
00:02:33.050 --> 00:02:35.450
set and see kind of what's inside this
data set.

53
00:02:35.450 --> 00:02:38.410
And then you'll have to explore all kinds
of things in a large data set.

54
00:02:38.410 --> 00:02:40.910
But if you can narrow down your interest,
your, your interest to a

55
00:02:40.910 --> 00:02:42.820
specific type of question, then that can

56
00:02:42.820 --> 00:02:45.510
be extremely useful for simplifying your
problem.

57
00:02:45.510 --> 00:02:47.520
So I encourage you to to kind of think
about what

58
00:02:47.520 --> 00:02:52.130
type of question you're interested in
answering before, you go into delving

59
00:02:52.130 --> 00:02:54.430
into all the details of your data set.

60
00:02:54.430 --> 00:02:56.990
So, the science, generally speaking, will
determine

61
00:02:56.990 --> 00:02:59.658
what type of question you're interested in
asking.

62
00:02:59.658 --> 00:03:01.912
And that will lead you to the data.

63
00:03:01.912 --> 00:03:04.085
Which may lead you to applied statistics,
which

64
00:03:04.085 --> 00:03:06.360
is you know, you use to analyze the data.

65
00:03:06.360 --> 00:03:08.090
And then if you get really you know,

66
00:03:08.090 --> 00:03:10.210
ambitious you might want to think of some
theoretical

67
00:03:10.210 --> 00:03:13.280
statistics that will kind of generalize
the the

68
00:03:13.280 --> 00:03:15.310
methods that you apply to different types
of data.

69
00:03:15.310 --> 00:03:17.330
Now of course there are relatively few
people

70
00:03:17.330 --> 00:03:21.500
who can even, who can do that, and so I,
that would not be expected of everyone.

71
00:03:21.500 --> 00:03:24.100
So the part that's in the, the red bracket
that's

72
00:03:24.100 --> 00:03:29.220
number one That's typically referred to as
statistical methods development.

73
00:03:29.220 --> 00:03:32.520
The part that's in the purple bracket here
number two, which is just kind

74
00:03:32.520 --> 00:03:37.810
of, the application of statistics to, just
to raw data without any sense of science.

75
00:03:37.810 --> 00:03:40.130
is, is what I would refer to as the danger

76
00:03:40.130 --> 00:03:43.254
zone, and which we, which I kind of,
derive here from.

77
00:03:43.254 --> 00:03:47.910
A kind of a Venn diagram of data science
that's written by Drew Conway.

78
00:03:47.910 --> 00:03:49.680
The idea is if you just kind of randomly
apply

79
00:03:49.680 --> 00:03:53.180
statistical methods to data sets to find
an interesting answer.

80
00:03:53.180 --> 00:03:57.250
First of all, you will find something
interesting almost certainly, but

81
00:03:57.250 --> 00:04:01.260
it may not be reproducible and it may not
be really meaningful.

82
00:04:01.260 --> 00:04:04.340
And so I think the a truly, a proper data
analysis

83
00:04:04.340 --> 00:04:08.350
has a scientific context, it hopefully has
at least some general

84
00:04:08.350 --> 00:04:09.760
question that we're trying to.

85
00:04:09.760 --> 00:04:11.900
To try and to investigate which will
narrow

86
00:04:11.900 --> 00:04:14.310
down the kind of dimensionality of the
problem.

87
00:04:14.310 --> 00:04:15.580
And then we'll apply the appropriate

88
00:04:15.580 --> 00:04:17.900
statistical methods to the appropriate
data.

89
00:04:20.740 --> 00:04:24.070
So, let's start with the very basic
example of a question.

90
00:04:24.070 --> 00:04:25.470
So a general question might be you know,

91
00:04:25.470 --> 00:04:28.780
can I automatically detect emails that are
spam?

92
00:04:28.780 --> 00:04:29.950
And those that are not.

93
00:04:29.950 --> 00:04:32.916
Of course, this is an important question
if, if you use email.

94
00:04:32.916 --> 00:04:35.380
If you want to know what are the emails
that you, that you should

95
00:04:35.380 --> 00:04:38.680
read, that are important, and what are the
emails that are just spam?

96
00:04:38.680 --> 00:04:41.290
And so you might want to, and so if you
want to turn that into

97
00:04:41.290 --> 00:04:45.130
a data analysis problem there are many
ways that you could answer this question.

98
00:04:45.130 --> 00:04:45.760
For example, you could

99
00:04:45.760 --> 00:04:50.240
just hire someone to just go through your
email and figure out what's spam or not.

100
00:04:50.240 --> 00:04:52.840
But that's not really that's probably not
very sustainable.

101
00:04:52.840 --> 00:04:54.180
It's not particularly efficient.

102
00:04:54.180 --> 00:04:55.550
So, if you want to turn this into a

103
00:04:55.550 --> 00:04:57.870
data analysis question, you have to make
the question

104
00:04:57.870 --> 00:05:00.730
a little bit more concrete and, and
translate it

105
00:05:00.730 --> 00:05:04.710
by using terms that are specific to data
analysis tools.

106
00:05:04.710 --> 00:05:06.852
And so a more concrete version of this
question might be

107
00:05:06.852 --> 00:05:08.490
you know, can I use quantitative

108
00:05:08.490 --> 00:05:10.970
characteristics of the emails themselves
to

109
00:05:10.970 --> 00:05:12.800
classify them as spam or ham.

110
00:05:12.800 --> 00:05:16.110
Okay so now we can start looking at emails
and try to think well what are these

111
00:05:16.110 --> 00:05:20.340
quantitative characteristics that I want
to develop so

112
00:05:20.340 --> 00:05:22.218
that I can kind of classify them as spam.

113
00:05:24.670 --> 00:05:26.679
So, once you've got a question here we
want to

114
00:05:26.679 --> 00:05:29.190
know okay, how do I separate out my email.

115
00:05:29.190 --> 00:05:32.440
So that I know what's spam and what's not
spam that way presumably you can kind

116
00:05:32.440 --> 00:05:36.810
of get rid of all the spam and just read
the usual, the, the, the real email.

117
00:05:36.810 --> 00:05:38.030
So the first thing you might want to think
about

118
00:05:38.030 --> 00:05:40.550
is what is the ideal data set for this
problem.

119
00:05:40.550 --> 00:05:43.480
And so if I had, you know, all the
resources

120
00:05:43.480 --> 00:05:46.690
in the world, what would I go out to look
for?

121
00:05:46.690 --> 00:05:49.419
And so there are different types of data
sets that you could potentially collect.

122
00:05:50.490 --> 00:05:54.500
And depending on the goal and the type of
question you, you're asking.

123
00:05:54.500 --> 00:05:56.230
A descriptive data set.

124
00:05:56.230 --> 00:05:57.730
So if you're looking, interested in a
descriptive

125
00:05:57.730 --> 00:06:00.530
problem, you might think of a whole
population.

126
00:06:00.530 --> 00:06:03.650
So again, just kind of.
So you don't need to sample anything.

127
00:06:03.650 --> 00:06:04.810
You might want to just get the entire

128
00:06:04.810 --> 00:06:06.470
census or population that you're looking
for.

129
00:06:06.470 --> 00:06:09.240
So all the emails in the universe, for
example.

130
00:06:09.240 --> 00:06:11.770
If you just want to explore your question.

131
00:06:11.770 --> 00:06:15.226
You might just take a random sample with a
bunch of variables measured.

132
00:06:15.226 --> 00:06:15.491
If you

133
00:06:15.491 --> 00:06:17.717
want to make inference about a problem
then you

134
00:06:17.717 --> 00:06:19.201
have to have you have yet to be very

135
00:06:19.201 --> 00:06:21.480
careful about the sampling mechanism and,
and the

136
00:06:21.480 --> 00:06:25.140
definition of the population that you're
sampling from because.

137
00:06:25.140 --> 00:06:29.560
Typically when you make an inferential
statement, you take you're, you're, you're

138
00:06:29.560 --> 00:06:33.290
drawing from a sample to make a conclusion
about a larger population.

139
00:06:33.290 --> 00:06:36.280
So there the sampling mechanism, it was
very important.

140
00:06:36.280 --> 00:06:38.730
If you want to make a prediction, then
you're going to need

141
00:06:38.730 --> 00:06:41.390
something like a training set and a test
data set

142
00:06:41.390 --> 00:06:43.400
from this, from a population that you're
interested in,

143
00:06:43.400 --> 00:06:46.332
so that you can build a model and a
classifier.

144
00:06:46.332 --> 00:06:49.350
If you want to make a causal statement, so
you want to

145
00:06:49.350 --> 00:06:53.040
say okay, if I modify this component, then
something else happens.

146
00:06:53.040 --> 00:06:55.570
So this is basically, you're going to need

147
00:06:55.570 --> 00:06:58.130
experimental data, and one type of
experimental

148
00:06:58.130 --> 00:06:59.930
data is from, from some, from something

149
00:06:59.930 --> 00:07:02.910
like a randomized trial or a randomized
study.

150
00:07:02.910 --> 00:07:04.640
And then if you want to make mechanistic
types

151
00:07:04.640 --> 00:07:06.505
of statements, you need data about all the
different

152
00:07:06.505 --> 00:07:08.655
components of the system that you're
trying to describe.

153
00:07:08.655 --> 00:07:10.910
[SOUND]

154
00:07:10.910 --> 00:07:13.760
So, for our problem here with spam one
ideal

155
00:07:13.760 --> 00:07:16.040
day so perhaps would be you know, if you
use

156
00:07:16.040 --> 00:07:18.650
Gmail you know that all the emails in the
Gmail

157
00:07:18.650 --> 00:07:21.130
system are going to be stored on Google's
data centers, right?

158
00:07:21.130 --> 00:07:22.550
So, why don't we just get.

159
00:07:22.550 --> 00:07:27.430
All the data in, in Google data centers,
all the emails in Google data centers.

160
00:07:27.430 --> 00:07:29.210
Right, because that would be a whole
population of

161
00:07:29.210 --> 00:07:31.530
emails, and then we can just kind of build

162
00:07:31.530 --> 00:07:34.740
our classifier based on all this data, and
then we have, we,

163
00:07:34.740 --> 00:07:37.970
we wouldn't have to worry about sampling,
because we'd have all the data.

164
00:07:37.970 --> 00:07:40.850
And then, and so that would be a, a kind
of an ideal data set.

165
00:07:42.380 --> 00:07:44.950
So, of course, in the real world, you have
to think

166
00:07:44.950 --> 00:07:47.520
about, well what are the data that you can
actually access, right?

167
00:07:47.520 --> 00:07:51.380
So, maybe someone at Google can actually,
can

168
00:07:51.380 --> 00:07:54.160
access all the emails that go through
Gmail.

169
00:07:54.160 --> 00:07:57.030
But, but even in that extreme case, it may
be difficult.

170
00:07:57.030 --> 00:08:00.030
And furthermore, most people are not
going to be able to access that.

171
00:08:00.030 --> 00:08:02.046
So, sometimes you, you have to go for

172
00:08:02.046 --> 00:08:05.170
something that's not quite the ideal data
set.

173
00:08:05.170 --> 00:08:08.150
And so you might be able to find free data
on the web.

174
00:08:08.150 --> 00:08:13.650
You might need to buy some data from a
provider And if you, and in these

175
00:08:13.650 --> 00:08:16.770
kinds of cases, you should be sure to
respect the terms of use for the data.

176
00:08:16.770 --> 00:08:20.080
So any agreement or contract that you
agree, that you've

177
00:08:20.080 --> 00:08:22.160
kind of agreed to about the data has to be

178
00:08:22.160 --> 00:08:22.720
adhered to.

179
00:08:23.830 --> 00:08:25.870
And if the data simply do not exist out
there,

180
00:08:25.870 --> 00:08:29.230
you may need to generate the data yourself
in some way.

181
00:08:29.230 --> 00:08:31.330
So, getting all the data from Google will

182
00:08:31.330 --> 00:08:34.750
probably not be possible, because most,
I'm guessing their

183
00:08:34.750 --> 00:08:37.150
data centers have some very high security,
and

184
00:08:37.150 --> 00:08:39.719
so we're going to have to go with
something else.

185
00:08:39.719 --> 00:08:42.774
And so one possible solution is the is, is
comes from the

186
00:08:42.774 --> 00:08:46.444
UCI machine on your repository, which is
the spam based data set.

187
00:08:46.444 --> 00:08:47.804
And this is a collection

188
00:08:47.804 --> 00:08:51.544
of spam that was, that was pur, and this
data set was created

189
00:08:51.544 --> 00:08:57.170
by people at Hewlett Packard who collected
some, a couple thousand spam messages.

190
00:08:57.170 --> 00:09:00.400
Spam and regular messages, and classified
them appropriately.

191
00:09:00.400 --> 00:09:03.040
So you can use this database to explore
your

192
00:09:03.040 --> 00:09:05.780
problem of how to classify emails into
spam or ham.

193
00:09:07.880 --> 00:09:10.540
So, when you obtain the data, the first
goal

194
00:09:10.540 --> 00:09:12.450
is to, you know, try to obtain the raw
data.

195
00:09:12.450 --> 00:09:15.510
For example, from the UCI machine on your
repository.

196
00:09:15.510 --> 00:09:18.220
You have to be careful to reference the
source, so wherever you get the data

197
00:09:18.220 --> 00:09:19.870
from, you should always reference the
source

198
00:09:19.870 --> 00:09:22.180
and keep track of where it came from.

199
00:09:22.180 --> 00:09:25.470
If you're asking, if you want, if you need
to get data from a person or an

200
00:09:25.470 --> 00:09:28.280
invest, investigator that you're not
familiar with often

201
00:09:28.280 --> 00:09:30.070
a very polite email will go a long way.

202
00:09:30.070 --> 00:09:32.458
They may be willing to share that data
with you.

203
00:09:32.458 --> 00:09:32.865
And if you,

204
00:09:32.865 --> 00:09:34.990
if you get data from an internet source,
you should

205
00:09:34.990 --> 00:09:38.800
always make sure at the very minimum
record the URL

206
00:09:38.800 --> 00:09:40.770
which is the website indicator of where
you got the

207
00:09:40.770 --> 00:09:43.310
data and the time and date that you access
that.

208
00:09:43.310 --> 00:09:46.250
So people have a reference, when that data
was available.

209
00:09:46.250 --> 00:09:51.070
In the future, the website might go down
or the URL may change or may not be

210
00:09:51.070 --> 00:09:53.700
available, but at least at that time you

211
00:09:53.700 --> 00:09:55.779
got that data you documented how you got
it.

212
00:09:58.210 --> 00:10:00.528
So the data set that we're going to talk
about

213
00:10:00.528 --> 00:10:03.395
in this example since we don't have access
to Google's

214
00:10:03.395 --> 00:10:05.652
data centers, is the spam in a data set
which

215
00:10:05.652 --> 00:10:07.900
you can get from the Kern Lab package in
R.

216
00:10:07.900 --> 00:10:11.100
So it comes with the Kern Lab package and
so if you

217
00:10:11.100 --> 00:10:14.340
install the Kern Lab package you can load
the data set right away.

218
00:10:15.820 --> 00:10:17.790
So the first thing that you typically need
to do

219
00:10:18.860 --> 00:10:21.344
with any data set is to clean it a little
bit.

220
00:10:21.344 --> 00:10:23.490
So often-raw data you typically

221
00:10:23.490 --> 00:10:27.580
need to be processed in some way to get it
into a form where you can model it

222
00:10:27.580 --> 00:10:30.990
or feed it into a modeling program If it's

223
00:10:30.990 --> 00:10:33.785
already pre-processed, it's important that
you understand how it was.

224
00:10:33.785 --> 00:10:36.670
Pre-processed so try to get some
documentation about

225
00:10:36.670 --> 00:10:38.860
what the pre-processing was and how it was
done.

226
00:10:39.890 --> 00:10:42.740
You have to understand kind of where the
data come from, so for example

227
00:10:42.740 --> 00:10:46.510
if it came from a survey, you need to know
how the sampling was done.

228
00:10:46.510 --> 00:10:48.670
it, was it a convenient sample, or was,

229
00:10:48.670 --> 00:10:50.850
did the data come from an observational
study, did it come

230
00:10:50.850 --> 00:10:54.490
from experiments of, the source of the
data is very important.

231
00:10:54.490 --> 00:10:57.900
You may need to reformat the data in a
certain way

232
00:10:57.900 --> 00:11:00.850
to get it to work in a certain type of
analysis.

233
00:11:00.850 --> 00:11:02.840
If the data set is extremely large you may
want

234
00:11:02.840 --> 00:11:05.750
to sub-sample the data set to make it more
manageable.

235
00:11:05.750 --> 00:11:09.020
And so anything you do to clean the data,
it is very important that you

236
00:11:09.020 --> 00:11:11.720
record these steps and write them down you

237
00:11:11.720 --> 00:11:13.870
know, in scripts or whatever is most
convenient.

238
00:11:13.870 --> 00:11:16.270
Because someone you or someone else is
going to have

239
00:11:16.270 --> 00:11:20.010
to reproduce these steps if they want to
reproduce your findings.

240
00:11:20.010 --> 00:11:23.440
And if you don't document all these
pre-processing steps, then

241
00:11:23.440 --> 00:11:25.670
no one will ever be able to do it again.

242
00:11:25.670 --> 00:11:27.500
So, once you've cleaned the data and
you've gotten

243
00:11:27.500 --> 00:11:30.830
a basic look at it, it's important to
determine of

244
00:11:30.830 --> 00:11:32.950
the data are good enough to solve your
problems

245
00:11:32.950 --> 00:11:35.460
because in some cases they may not be good
enough.

246
00:11:35.460 --> 00:11:38.920
You may not have enough data, you may not
have enough variables or enough

247
00:11:38.920 --> 00:11:43.520
characteristics, the sampling of the data
may be inappropriate for your question.

248
00:11:43.520 --> 00:11:45.300
So there may be all kinds of problems that
occur

249
00:11:45.300 --> 00:11:48.760
and you, that you realize as you clean the
data.

250
00:11:48.760 --> 00:11:50.300
And so, and if you determined the data

251
00:11:50.300 --> 00:11:52.650
are not good enough for your question,
then you've

252
00:11:52.650 --> 00:11:54.620
got to quit, and, or, and try again,

253
00:11:54.620 --> 00:11:57.380
or change the data, or try a different
question.

254
00:11:57.380 --> 00:12:01.050
it's, it's important to not to just push
on, and with the data

255
00:12:01.050 --> 00:12:04.540
you have, just because that's all that
you've got, because that can lead

256
00:12:04.540 --> 00:12:06.420
inappropriate inferences or conclusions.

257
00:12:08.790 --> 00:12:11.730
So here is our cleaned data set that we're
going to use for this example.

258
00:12:11.730 --> 00:12:13.960
It's already been cleaned for us, in the
Kern Lab

259
00:12:13.960 --> 00:12:16.730
package, and I'm just showing you the
first five variables here.

260
00:12:16.730 --> 00:12:19.430
There are many other variables in the data
set but

261
00:12:19.430 --> 00:12:23.480
you can see that there's 4601 observations
of the five variables.

262
00:12:23.480 --> 00:12:25.760
I put the link here, you can see the help
page in turn,

263
00:12:25.760 --> 00:12:28.840
it shows you where the data set came from
and how it's processed.