WEBVTT

1
00:00:00.740 --> 00:00:03.800
Perhaps the most important
theorem in all of statistics is

2
00:00:03.800 --> 00:00:06.200
the so-called Central Limit Theorem.

3
00:00:06.200 --> 00:00:07.060
For our purposes,

4
00:00:07.060 --> 00:00:10.850
the Central Limit Theorem states that
the distribution of averages of iid random

5
00:00:10.850 --> 00:00:14.930
variables becomes that of a standard
normal as the sample size increases.

6
00:00:15.990 --> 00:00:20.020
Because it has fairly loose requirements
on the collection of populations that it

7
00:00:20.020 --> 00:00:23.520
applies to, the Central Limit Theorem
applies in a nearly endless variety of

8
00:00:23.520 --> 00:00:24.929
settings, and we'll go through several.

9
00:00:26.070 --> 00:00:29.780
The basic result is that, if we take
an estimate like the sample average,

10
00:00:29.780 --> 00:00:35.140
x bar, Subtract off its population mean,
mu, and divide by its standard error,

11
00:00:35.140 --> 00:00:36.360
sigma over square root n.

12
00:00:37.540 --> 00:00:42.090
That random variable limits
to that of a standard normal.

13
00:00:42.090 --> 00:00:46.710
I would also add that replacing
the population standard deviation,

14
00:00:46.710 --> 00:00:49.720
which is unknown by the sample
standard deviation,

15
00:00:49.720 --> 00:00:53.350
which is known does not change
the Central Limit Theorem.

16
00:00:53.350 --> 00:00:56.380
The most useful way we think about
the Central Limit Theorem is to say that

17
00:00:56.380 --> 00:01:00.810
the sample average is approximately
normally distributed with a mean given by

18
00:01:00.810 --> 00:01:04.340
the population mean and a variance given
by the standard error of the mean.

19
00:01:05.430 --> 00:01:09.910
So let's go through several examples
to illustrate the Central Limit Theorem

20
00:01:09.910 --> 00:01:10.990
using simulation.

21
00:01:12.020 --> 00:01:13.640
First, lets start with a standard die.

22
00:01:15.060 --> 00:01:17.760
What's interesting about this
conceptual experiment and

23
00:01:17.760 --> 00:01:22.810
the simulation that we're going to
conduct is that imagine if you had to

24
00:01:22.810 --> 00:01:27.250
simulate random normals prior
to the advent of computers.

25
00:01:27.250 --> 00:01:30.360
For example, if you were a statistician
working at that time, and

26
00:01:30.360 --> 00:01:33.910
you wanted to evaluate the behavior of
something like the ki squared statistic,

27
00:01:33.910 --> 00:01:36.130
which is a function of
normal random variables.

28
00:01:38.190 --> 00:01:42.260
So let's let out,
Xi be the outcome for die i.

29
00:01:42.260 --> 00:01:47.540
And remember that the mean of
the distribution of die rolls is 3.5 and

30
00:01:47.540 --> 00:01:50.077
that its variance is 2.92, so

31
00:01:50.077 --> 00:01:55.110
the standard error of the mean
is square root 2.92 over n.

32
00:01:55.110 --> 00:01:59.574
Lets roll n dice, take their mean,
subtract off 3.5, and

33
00:01:59.574 --> 00:02:02.789
divide by 1.71 over square root n.

34
00:02:03.990 --> 00:02:06.040
And repeat this over and over again.

35
00:02:06.040 --> 00:02:07.870
If the Central Limit Theorem is right,

36
00:02:07.870 --> 00:02:10.870
this should look exactly
like a standard bell curve.

37
00:02:10.870 --> 00:02:13.720
So just to remind
ourselves what's happening?

38
00:02:13.720 --> 00:02:17.300
We roll the dice 10 times
in this first panel.

39
00:02:17.300 --> 00:02:23.720
We subtracted off 3.5 and we divided
by 1.71 divided by square root n.

40
00:02:23.720 --> 00:02:27.030
Repeated that process over,
and over, and over again.

41
00:02:27.030 --> 00:02:31.270
And the histogram displays
the distribution of the different

42
00:02:31.270 --> 00:02:32.780
normalized averages that we got.

43
00:02:35.160 --> 00:02:38.060
We know already that this
distribution has to be

44
00:02:38.060 --> 00:02:41.370
centered around zero because we've
subtracted off the mean 3.5.

45
00:02:41.370 --> 00:02:44.350
So it has to be centered around zero.

46
00:02:44.350 --> 00:02:47.250
And we know exactly, what the variance
of this distribution has to

47
00:02:47.250 --> 00:02:50.410
be were we to repeat this
process infinitely many times.

48
00:02:51.900 --> 00:02:55.230
However, what the Central Limit Theorem
is telling us, is also the shape.

49
00:02:55.230 --> 00:02:58.200
And the shape has to be
like that of a bell curve.

50
00:02:58.200 --> 00:03:01.990
And because we normalized the data, it has
to be exactly that of a standard normal.

51
00:03:01.990 --> 00:03:05.490
And we see that the approximation
is actually very good, even for

52
00:03:05.490 --> 00:03:07.270
just ten die rolls.

53
00:03:07.270 --> 00:03:10.180
For 20, it gets even better,
and for 30, it's better still.

54
00:03:11.970 --> 00:03:14.010
Flipping a coin is an interesting case,

55
00:03:14.010 --> 00:03:18.940
because it goes back to the original
development of the Central Limit Theorem,

56
00:03:18.940 --> 00:03:22.010
as an approximation for
the distribution of sample proportions.

57
00:03:23.060 --> 00:03:28.050
So let's let Xi be the 0 or 1 result of
the ith flip of a possibly unfair coin.

58
00:03:29.280 --> 00:03:33.500
Then recall we usually give the notation
p-hat, as the sample proportion.

59
00:03:33.500 --> 00:03:35.400
Which is simply the mean or the average,

60
00:03:35.400 --> 00:03:39.250
the empirical average of the coin flip's
coding 0 as a tail and 1 is a head.

61
00:03:40.660 --> 00:03:43.840
Recall that the expected
value of Xi is p and

62
00:03:43.840 --> 00:03:46.170
that the variance of Xi
is p times 1 minus p.

63
00:03:46.170 --> 00:03:52.100
Then the standard error of the mean, or
the standard error of the proportion

64
00:03:52.100 --> 00:03:56.200
of heads p-hat is square root
p times 1 minus p over n.

65
00:03:57.990 --> 00:04:03.150
So if we take the statistic p-hat,
subtract off its population mean, and

66
00:04:03.150 --> 00:04:07.280
divide by its standard error,
square root p times 1 minus p over n

67
00:04:07.280 --> 00:04:11.240
that should be approximately normally
distributed if n is large enough.

68
00:04:12.920 --> 00:04:15.299
Now, notice, if the coin is fair,
then p is one-half.

69
00:04:16.650 --> 00:04:21.515
And p times 1 minus p is a quarter,
which as a square root is one-half.

70
00:04:22.590 --> 00:04:27.480
So the standard error for a fair
coin flip is 1 over 2 square root n.

71
00:04:27.480 --> 00:04:31.590
So let's flip the coin n times,
take the sample proportion of heads,

72
00:04:31.590 --> 00:04:36.670
subtract off 0.5 and
multiply the result by 2 square root n.

73
00:04:36.670 --> 00:04:38.810
Here's the result of our simulations.

74
00:04:38.810 --> 00:04:41.580
Consider the panel with ten
coin flips here on the left.

75
00:04:43.040 --> 00:04:47.097
This is the result of flipping
the coin ten times, subtracting off

76
00:04:47.097 --> 00:04:51.816
the population mean 0.5, and dividing
by the standard error of the mean.

77
00:04:51.816 --> 00:04:55.540
Square root p times 1 minus p over n.

78
00:04:55.540 --> 00:04:57.630
Doing this over, and over, and over again.

79
00:04:57.630 --> 00:04:58.680
We get a good sense for

80
00:04:58.680 --> 00:05:03.150
what the distribution of normalized
proportions of ten coin flips looks like.

81
00:05:04.240 --> 00:05:07.280
It's centered at zero because
we subtracted off the mean and

82
00:05:07.280 --> 00:05:11.660
its variance is governed by
the p times 1 minus p over n.

83
00:05:11.660 --> 00:05:14.690
What this lecture is telling us is
that the distribution should be

84
00:05:14.690 --> 00:05:17.750
approximately that of a standard normal.

85
00:05:17.750 --> 00:05:20.640
And for reference, I drew the density
of the standard normal here.

86
00:05:23.010 --> 00:05:25.920
You can see some of the discreteness,
there's only two levels for

87
00:05:25.920 --> 00:05:29.640
a coin, so the ten, the average of
ten coin flips could only take so

88
00:05:29.640 --> 00:05:34.780
many different combinations, and so some
of the discreteness is showing through.

89
00:05:34.780 --> 00:05:35.950
But when we get to 20 or

90
00:05:35.950 --> 00:05:39.390
30 coin flips, we could see that
it actually looks quite Gaussian.

91
00:05:39.390 --> 00:05:44.860
I would like to emphasize that
the speed at which the normalized coin

92
00:05:44.860 --> 00:05:51.030
flips converges to normality is
governed by how biased the coin is.

93
00:05:51.030 --> 00:05:52.030
For example here,

94
00:05:52.030 --> 00:05:56.200
I show what the simulations look like for
a probability of head 0.9.

95
00:05:56.200 --> 00:06:02.040
You can see for ten, that distribution
does not look very bell shaped.

96
00:06:02.040 --> 00:06:06.560
By 30, it's getting there but still
the probability's when approximated by

97
00:06:06.560 --> 00:06:08.470
the normal distribution
would not be perfect.

98
00:06:09.940 --> 00:06:14.100
So just keep this in mind that the central
limit theorem doesn't guarantee that

99
00:06:14.100 --> 00:06:16.950
the normal distribution will
be a good approximation.

100
00:06:16.950 --> 00:06:21.380
Simply that as the number of coin
flips limits to infinity eventually,

101
00:06:21.380 --> 00:06:23.040
it will be a good approximation.

102
00:06:24.400 --> 00:06:27.790
As a fun aside,
I wanted to talk about Galton's quincunx.

103
00:06:27.790 --> 00:06:31.496
A machine that you might have seen
if you visited a science museum.

104
00:06:31.496 --> 00:06:36.330
Basically, the cunx of this machine is
illustrating the Central Limit Theorem

105
00:06:36.330 --> 00:06:39.550
with a game that looks
a little like Pachinko.

106
00:06:39.550 --> 00:06:41.190
So here is an exampled.

107
00:06:41.190 --> 00:06:45.400
What, what would happen is
a ball would come down here and

108
00:06:45.400 --> 00:06:49.180
it would fall down here,
and it would hit this peg.

109
00:06:50.350 --> 00:06:53.870
And then it would fully go left or
right with equal probability.

110
00:06:55.000 --> 00:06:56.040
And then it would go down.

111
00:06:56.040 --> 00:06:59.170
Let's say it, it went in this
direction and then it went down there.

112
00:06:59.170 --> 00:07:02.080
And then it would fall left or
right with equal probability.

113
00:07:03.640 --> 00:07:07.980
And eventually, the ball would
work its way down to a bin, and

114
00:07:07.980 --> 00:07:09.430
it would get collected down here.

115
00:07:10.620 --> 00:07:15.330
You can think of each time the ball
hits a peg and has to choose left or

116
00:07:15.330 --> 00:07:18.590
right, as a binomial experiment,
a coin flip.

117
00:07:18.590 --> 00:07:21.950
If it were to get all successes,
let's say,

118
00:07:21.950 --> 00:07:24.080
it would head all the way
over in this direction and

119
00:07:24.080 --> 00:07:27.700
if it were to get all failures, it would
head all the way over in that direction.

120
00:07:27.700 --> 00:07:29.620
It would be like flipping a coin.

121
00:07:29.620 --> 00:07:32.430
So each row is like another coin flip.

122
00:07:35.010 --> 00:07:39.750
By virtue of the Central Limit Theorem,
we know that the proportion of heads from

123
00:07:39.750 --> 00:07:43.800
a bunch of a coin flips is
approximately normally distributed.

124
00:07:43.800 --> 00:07:48.910
So, we could multiply that by n and
then also conclude that the sum,

125
00:07:48.910 --> 00:07:52.560
the total number of heads will also be
approximately normally distributed.

126
00:07:52.560 --> 00:07:56.990
And so what you'll see at the science
museum is that the balls will collect down

127
00:07:56.990 --> 00:08:02.720
here, and they'll actually draw the
relevant normal distribution, right there.

128
00:08:02.720 --> 00:08:09.190
And then you'll see, as the balls collect,
they'll collect up into this in exactly,

129
00:08:09.190 --> 00:08:11.930
overlay this superimposed
normal distribution.

130
00:08:11.930 --> 00:08:15.430
And then when it fills up to that point
there's usually a little thing that,

131
00:08:15.430 --> 00:08:18.470
that drops all the balls down and
it starts over again.

132
00:08:18.470 --> 00:08:21.700
So, at any rate it's a, it's a fun idea,
Y\you can Google online and

133
00:08:21.700 --> 00:08:24.185
see some examples of Galton's quincunx.

134
00:08:24.185 --> 00:08:27.230
The fun application of
the Central Limit Theorem.