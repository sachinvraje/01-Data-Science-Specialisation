WEBVTT

1
00:00:01.200 --> 00:00:04.200
Hi, my name is Brian Caffo and this is the
multiple testing

2
00:00:04.200 --> 00:00:09.910
lecture in the statistical inference class
in the course area data science series.

3
00:00:09.910 --> 00:00:13.720
This class is co-taught by my
collaborators Jeff Leek and Roger Peng.

4
00:00:13.720 --> 00:00:15.470
We're all in the department of bio
statistics

5
00:00:15.470 --> 00:00:18.490
in the Johns Hopkins Bloomberg School of
Public Health.

6
00:00:18.490 --> 00:00:22.280
And in fact, today, we have a very special
guest lecturer, Jeff Leek.

7
00:00:23.440 --> 00:00:29.200
I don't have a picture of Jeff so lets
just assume he kind of looks like this.

8
00:00:30.650 --> 00:00:33.220
You know, he sets things like Internet's
rule.

9
00:00:36.070 --> 00:00:39.102
And so, I'll just let him take over from,
from there.

10
00:00:39.102 --> 00:00:41.173
[SOUND].

11
00:00:41.173 --> 00:00:43.740
>> This video is about multiple testing.

12
00:00:43.740 --> 00:00:46.160
We talked about hypothesis testing early
in the course

13
00:00:46.160 --> 00:00:49.570
and I mentioned that we when you perform
more than

14
00:00:49.570 --> 00:00:51.880
one hypothesis test you have to do some
sort

15
00:00:51.880 --> 00:00:54.870
of correction to make sure that you're not
fooling yourself.

16
00:00:54.870 --> 00:00:58.460
This lecture is a little bit about how to
do these corrections.

17
00:00:58.460 --> 00:01:00.850
The key ideas here are that hypothesis
testing in significant analysis.

18
00:01:00.850 --> 00:01:02.650
Are commonly overused techniques.

19
00:01:02.650 --> 00:01:04.650
In particular, what people will often do

20
00:01:04.650 --> 00:01:07.245
is calcula, calculate multiple P values
when

21
00:01:07.245 --> 00:01:11.280
analysing the same data set, and they will
only report the smallest P value

22
00:01:11.280 --> 00:01:14.538
or report all of the P values but claim
all P values less than

23
00:01:14.538 --> 00:01:16.920
.05 are significant which leads to some

24
00:01:16.920 --> 00:01:19.060
problems that Iâ€™ll demonstrate in a
minute.

25
00:01:20.170 --> 00:01:22.890
So, what we would like to do is correct
for multiple testing

26
00:01:22.890 --> 00:01:24.750
to avoid false positives or false

27
00:01:24.750 --> 00:01:27.660
discoveries when forming analysis with
many variables.

28
00:01:27.660 --> 00:01:32.880
There are two key components to multiple
testing corrections, first it's

29
00:01:32.880 --> 00:01:35.360
a definition of an error measure that you
would like to control.

30
00:01:36.390 --> 00:01:39.290
Then a, a definition of a correction or a

31
00:01:39.290 --> 00:01:42.370
statistical method that's used to control
that error mesh, measure.

32
00:01:44.320 --> 00:01:47.680
So this is related to three errors of
statics, which appears

33
00:01:47.680 --> 00:01:51.620
in this book by Brad Efron, a professor of
statistics at Stanford.

34
00:01:51.620 --> 00:01:55.020
So he talked about the three errors as,
the first error being when

35
00:01:55.020 --> 00:01:59.720
huge census level datasets were brought to
bear on simple but important questions.

36
00:01:59.720 --> 00:02:02.790
But it just collected a lot of data to try
to describe a population.

37
00:02:02.790 --> 00:02:07.740
Then the classical period of statistics
developed a theory of optimal inference

38
00:02:07.740 --> 00:02:10.950
for ringing as much information as
possible out of small sample sizes.

39
00:02:10.950 --> 00:02:13.890
This is back when data was very expensive
or difficult to collect.

40
00:02:15.620 --> 00:02:18.360
The third era, the era that we're in now,
is the era

41
00:02:18.360 --> 00:02:22.140
of scientific mass production, when data
is cheap and easy to collect.

42
00:02:22.140 --> 00:02:25.270
But this also means that we're performing
more and more analysis.

43
00:02:25.270 --> 00:02:28.020
And if we don't correct for the fact that
all of these analyses are performed,

44
00:02:28.020 --> 00:02:29.930
we're allowing for a small amount of error

45
00:02:29.930 --> 00:02:31.880
in each analysis, those errors can pile
up.

46
00:02:31.880 --> 00:02:36.690
So, the reason for performing multiple
testing corrections are because of

47
00:02:36.690 --> 00:02:40.320
these new technologies that are leading to
this increase in data.

48
00:02:40.320 --> 00:02:41.770
These technologies range from next

49
00:02:41.770 --> 00:02:44.288
generation sequencing machines in
molecular biology,

50
00:02:44.288 --> 00:02:50.470
to imaging of patients in clinical
studies, or electronic medical records.

51
00:02:50.470 --> 00:02:53.208
Or personalized or individualized
quantitative

52
00:02:53.208 --> 00:02:55.390
self-measurements that you might take

53
00:02:55.390 --> 00:02:57.910
with something like the Nike Fuel Band or
the FitBit.

54
00:03:00.280 --> 00:03:01.840
So why should we correct for multiple
tests?

55
00:03:01.840 --> 00:03:05.600
This is actually a cartoon to describe the
key problem that you run into.

56
00:03:05.600 --> 00:03:08.950
So suppose that you were looking at a
particular analysis

57
00:03:08.950 --> 00:03:11.320
where you wanted to see if jelly beans
cause acne.

58
00:03:12.340 --> 00:03:15.610
So what you do is you send off a bunch of
scientists to investigate.

59
00:03:15.610 --> 00:03:19.530
And they first look at just all jelly
beans.

60
00:03:19.530 --> 00:03:24.540
And they look and see if people eat any
kind of jellybeans, whether

61
00:03:24.540 --> 00:03:28.320
they get acne or not and their p value
comes back greater than .05.

62
00:03:28.320 --> 00:03:29.850
And so, the next thing that they might do
is go

63
00:03:29.850 --> 00:03:33.430
and run and tech test all the different
colors of jellybeans individually.

64
00:03:33.430 --> 00:03:36.500
Say, well, is there a relationship between
purple jelly beans

65
00:03:36.500 --> 00:03:38.510
and acne brown jelly beans and acne and so
forth.

66
00:03:39.630 --> 00:03:41.010
And in each case they get a p greater

67
00:03:41.010 --> 00:03:44.590
than 0.05, so they don't report a
significant result.

68
00:03:44.590 --> 00:03:49.280
Until finally after they test over 20
different kinds of

69
00:03:49.280 --> 00:03:52.180
jelly beans they come up with one that is
significantly associated

70
00:03:52.180 --> 00:03:56.080
with acne, green jelly beans and they say
there's only

71
00:03:56.080 --> 00:04:00.140
a 5% chance of coinci, of this coincidence
occurring by chance.

72
00:04:00.140 --> 00:04:04.500
But it turns out that they tested 20
different hypotheses, so it was almost

73
00:04:04.500 --> 00:04:10.060
it became very likely that at least one of
them would result in a coincidence.

74
00:04:10.060 --> 00:04:12.270
So in other words, we allow, if we allow
for 5%

75
00:04:12.270 --> 00:04:16.040
chance of error in every hypothesis test
that we perform, and we

76
00:04:16.040 --> 00:04:19.790
perform at least 20 hypothesis tests then
we expect to find at

77
00:04:19.790 --> 00:04:23.370
least one error because 20 times 5 percent
is about 100 percent.

78
00:04:23.370 --> 00:04:26.770
So I've been referring to p values and
hypothesis testing

79
00:04:26.770 --> 00:04:31.220
as sort of interchangeable ideas but
they're not really interchangeable.

80
00:04:31.220 --> 00:04:33.432
So you can imagine where you're performing
a

81
00:04:33.432 --> 00:04:36.490
hypothesis test for para, parameter beta
and you're

82
00:04:36.490 --> 00:04:39.480
trying to determine whether is equals zero
versus

83
00:04:39.480 --> 00:04:41.400
the alternative that it does not equal to
zero.

84
00:04:42.420 --> 00:04:44.720
>> And so this is an example of when that
might happen

85
00:04:44.720 --> 00:04:45.890
is, say, you're fitting a linear

86
00:04:45.890 --> 00:04:48.780
regression model relating one variable to
another.

87
00:04:48.780 --> 00:04:52.920
If the coefficient for, the variable
that's the,

88
00:04:52.920 --> 00:04:56.350
covariant is equal to zero, then there's
no estimated

89
00:04:56.350 --> 00:04:59.010
association between the two variables, and
if it's

90
00:04:59.010 --> 00:05:01.089
not equal to zero, then there is some
association.

91
00:05:02.730 --> 00:05:04.910
So you might fit that linear progression
model

92
00:05:04.910 --> 00:05:08.220
and calculate a p value, as we've
discussed previously.

93
00:05:08.220 --> 00:05:10.310
And then to perform a hypothesis test, you
might look at

94
00:05:10.310 --> 00:05:15.080
that p value and calculate whether it's
less than some particular threshold.

95
00:05:15.080 --> 00:05:17.780
And if it is less than the threshold, then
you would say that.

96
00:05:17.780 --> 00:05:19.860
Beta does not equal to zero and if it's

97
00:05:19.860 --> 00:05:21.640
above the threshold you might say beta
equals zero.

98
00:05:21.640 --> 00:05:23.210
This is called the hypothesis test.

99
00:05:23.210 --> 00:05:26.360
When you're performing a hypothesis test,
this is

100
00:05:26.360 --> 00:05:30.040
the table of the possible outcomes that
can happen.

101
00:05:30.040 --> 00:05:33.560
So, in each row you have a particular
claim that you might make.

102
00:05:33.560 --> 00:05:36.420
Whether beta is equal to zero or beta is
not equal to zero.

103
00:05:37.860 --> 00:05:41.640
And then in each column you might have the
true state of the

104
00:05:41.640 --> 00:05:45.460
world where beta is equal to zero or beta
is no equal to zero.

105
00:05:45.460 --> 00:05:48.930
So, if you perform lots of hypothesise
tests, all the times when

106
00:05:48.930 --> 00:05:51.560
you say beta is equal to zero, and beta
actually is equal

107
00:05:51.560 --> 00:05:54.570
to zero fall into this cell, and all the
times when you

108
00:05:54.570 --> 00:05:57.890
say beta is not equal to zero, and is not
equal to zero.

109
00:05:57.890 --> 00:06:00.120
And it really not equal to zero but they
fall into this cell.

110
00:06:01.300 --> 00:06:05.580
So then there are two types of errors that
you might make, so the first one type

111
00:06:05.580 --> 00:06:10.100
one errors or false positives are the case
where you say beta is not equal to zero.

112
00:06:10.100 --> 00:06:13.700
In other words you say there is some
relationship between the variables.

113
00:06:13.700 --> 00:06:15.130
But there actually is not a relationship.

114
00:06:15.130 --> 00:06:18.440
So we're going to denote by v the number
of times that happens.

115
00:06:19.890 --> 00:06:22.180
The other type of error, type two errors
or false

116
00:06:22.180 --> 00:06:26.380
negatives are the cases where you might
claim that there's no

117
00:06:26.380 --> 00:06:29.080
that beta's equal to zero, in other words
there's no relationship

118
00:06:29.080 --> 00:06:32.500
between the variables, but it turns out
that there actually is.

119
00:06:32.500 --> 00:06:34.270
In general people tend to focus a little
bit

120
00:06:34.270 --> 00:06:36.810
more on type one errors, or false
positives when performing

121
00:06:36.810 --> 00:06:39.660
scientific investigations as we want to
limit the number of

122
00:06:39.660 --> 00:06:43.340
times that we're led astray, or we find
false positives.

123
00:06:43.340 --> 00:06:46.100
But in general the two error rates might

124
00:06:46.100 --> 00:06:50.700
be compared a different amount depending
on what type

125
00:06:50.700 --> 00:06:52.610
of problem that you're looking at and
whether one

126
00:06:52.610 --> 00:06:53.950
type of error is more costly than the
other.

127
00:06:55.180 --> 00:06:56.620
So in multiple testing there are a couple

128
00:06:56.620 --> 00:06:58.680
of different error rates that we might
consider.

129
00:06:58.680 --> 00:07:01.000
That's the first component of a multiple
testing procedure.

130
00:07:02.060 --> 00:07:05.360
So the error rate in this case it might be
the false positive

131
00:07:05.360 --> 00:07:09.610
rate, so this is just a rate which false
results are called significant.

132
00:07:09.610 --> 00:07:11.590
So in other words these are the results,
beta

133
00:07:11.590 --> 00:07:14.880
equals zero where there's no relationship
between the variables.

134
00:07:14.880 --> 00:07:16.560
What rate do we call them significant?

135
00:07:16.560 --> 00:07:18.380
This is just the average.

136
00:07:18.380 --> 00:07:20.970
Fraction of the times that we call them
significant when they're

137
00:07:20.970 --> 00:07:27.020
not, divided by the total number of not
significant variables [COUGH].

138
00:07:27.020 --> 00:07:30.200
Then there's another error measure which
is called the family wise error rate.

139
00:07:30.200 --> 00:07:34.600
And so, that is just the probability of at
least one false positive.

140
00:07:34.600 --> 00:07:37.900
So, this V variable counts all the times
where there's no

141
00:07:37.900 --> 00:07:41.340
relationship between the variables but we
claim that there is one.

142
00:07:41.340 --> 00:07:44.620
So if we do that V times, the family wise
error rate is controlling the

143
00:07:44.620 --> 00:07:47.590
probability that the number that we Make
that

144
00:07:47.590 --> 00:07:49.780
false claim greater than or equal to one.

145
00:07:49.780 --> 00:07:54.120
The false discovery rate is a little bit
different then

146
00:07:54.120 --> 00:07:56.430
the false positive rate in that it's the
rate at which

147
00:07:56.430 --> 00:08:00.380
claims significance are false in other
words r of the times

148
00:08:00.380 --> 00:08:02.920
we're going to claim that beta is not
equal to zero.

149
00:08:04.130 --> 00:08:07.480
And V of the time we're going to be wrong
about that decision.

150
00:08:07.480 --> 00:08:11.010
So E, the expected value of E divided by R
is actually

151
00:08:11.010 --> 00:08:14.730
just the rate at which our claims that
there's a relationship are false.

152
00:08:14.730 --> 00:08:17.000
So this is the false discovery rate versus
the

153
00:08:17.000 --> 00:08:19.170
false positive rate, which is the rate at
which.

154
00:08:20.390 --> 00:08:23.780
Actually truly false results are called

155
00:08:26.310 --> 00:08:26.626
true.

156
00:08:26.626 --> 00:08:29.140
So, the posi, false positive rate is
closely related to the

157
00:08:29.140 --> 00:08:31.650
type one error rate and it's actually kind
of a subtle distinction.

158
00:08:31.650 --> 00:08:33.130
And if you want to learn a little more
about

159
00:08:33.130 --> 00:08:35.880
that I have a link to a Wikipedia page
here.

160
00:08:35.880 --> 00:08:38.570
So the next part of multiple testing is
now that

161
00:08:38.570 --> 00:08:41.620
we've defined the different error
measures, how do we actually

162
00:08:43.670 --> 00:08:46.440
define a procedure that can be used to
control that error measure.

163
00:08:46.440 --> 00:08:50.160
In other words, is there a way that we
could perform a prof, procedure such that

164
00:08:50.160 --> 00:08:54.719
the rate of errors defined by that error
rate is held in check in a particular way.

165
00:08:56.230 --> 00:08:58.870
So, first we're going to talk about
controlling the false positive rate.

166
00:08:58.870 --> 00:09:01.790
And so, if P values are correctly
calculated, you

167
00:09:01.790 --> 00:09:04.040
can actually just use the P values that
you've

168
00:09:04.040 --> 00:09:07.350
calculated directly and call all P values
less than

169
00:09:07.350 --> 00:09:10.490
sum threshold Alpha, where Alpha's between
zero and one.

170
00:09:10.490 --> 00:09:13.500
To be significant, that will actually
control the

171
00:09:13.500 --> 00:09:15.700
false positive rate at level alpha on
average.

172
00:09:15.700 --> 00:09:21.170
In other words, the expected rate of false
positives is less than alpha.

173
00:09:21.170 --> 00:09:22.240
So here's the problem with that.

174
00:09:22.240 --> 00:09:25.770
Suppose that you perform say 10,000
hypothesis tests, this seems

175
00:09:25.770 --> 00:09:28.380
a little bit extreme, a large number of
tests maybe.

176
00:09:28.380 --> 00:09:30.110
For people that are doing just one or two

177
00:09:30.110 --> 00:09:34.370
regressions but in many high dimensional
settings or signal

178
00:09:34.370 --> 00:09:38.490
processing settings there are, this is
actually a reasonably

179
00:09:38.490 --> 00:09:40.880
small number of hypothesis tests that
might be performed

180
00:09:41.970 --> 00:09:44.475
and if you call all P values less than

181
00:09:44.475 --> 00:09:46.595
0.05 say significance and we say alpha
equal to

182
00:09:46.595 --> 00:09:49.990
0.05, then the expected number of false
positives is

183
00:09:49.990 --> 00:09:52.800
just the total number of tests that you've
performed.

184
00:09:52.800 --> 00:09:57.990
Times the false positive rate that you're
controlling the, error rate at.

185
00:09:57.990 --> 00:09:59.530
And so you get 500 false positives.

186
00:09:59.530 --> 00:10:05.050
So if you perform this many hypothesis
tests and you get 500 significant results,

187
00:10:05.050 --> 00:10:06.870
it's pretty likely that they're mostly
going

188
00:10:06.870 --> 00:10:09.480
to be made up of false positive results.

189
00:10:09.480 --> 00:10:12.540
So a question that we immediately comes to
mind is how do we

190
00:10:12.540 --> 00:10:16.280
control, a different error rate so that we
avoid so many false positives.

191
00:10:17.520 --> 00:10:19.270
So the first choice is the family-wise
error rate.

192
00:10:19.270 --> 00:10:21.950
And I talked about that just a minute ago,
which is we want to

193
00:10:21.950 --> 00:10:27.870
be able to control the probability that
we're going to make even one error.

194
00:10:27.870 --> 00:10:30.780
So the Bonferroni correction is actually
the approach for doing this.

195
00:10:30.780 --> 00:10:32.290
And I've linked to the Wikipedia page
here.

196
00:10:32.290 --> 00:10:34.730
It's actually the oldest multiple testing
correction.

197
00:10:35.740 --> 00:10:39.600
>> And the basic idea is that if you are
doing m hypothesis tests

198
00:10:39.600 --> 00:10:42.740
and you want to control the family wise
error rate at level alpha, in other

199
00:10:42.740 --> 00:10:46.590
words we want to make sure, try to ensure
that the probability of making

200
00:10:46.590 --> 00:10:52.050
even one error is even less than alpha, so
that's quite a stringent control there.

201
00:10:52.050 --> 00:10:55.008
We can calculate all the p values
normally.

202
00:10:55.008 --> 00:10:58.920
And then [UNKNOWN], take the alpha level
that we originally had for a single

203
00:10:58.920 --> 00:11:01.440
hypothesis test and divide it by the

204
00:11:01.440 --> 00:11:03.260
number of hypothesis tests that we
performed.

205
00:11:03.260 --> 00:11:08.190
In other words if alpha is 0.05 and the
number of hypothesis tests

206
00:11:08.190 --> 00:11:14.150
is ten then we get 0.05 divided by 10 is
equal to 0.005.

207
00:11:14.150 --> 00:11:17.010
So then we get this new alpha level, and
we count,

208
00:11:17.010 --> 00:11:21.930
call, all p values less thanum, this new
alpha level significant.

209
00:11:21.930 --> 00:11:25.050
Then that will, on average, control the
family wise error rate.

210
00:11:26.190 --> 00:11:28.000
The pros of this method are that it's easy

211
00:11:28.000 --> 00:11:30.766
to calculate and you don't make a lot of
errors.

212
00:11:30.766 --> 00:11:35.780
This is guaranteed to make very few errors
in the sense that this error rate,

213
00:11:35.780 --> 00:11:37.710
which is the probability of even one false

214
00:11:37.710 --> 00:11:40.330
positive, is actually controlled to be
quite low.

215
00:11:41.360 --> 00:11:43.760
A con, though, is that it also may be
very, very conservative.

216
00:11:43.760 --> 00:11:46.270
In other words, if you're doing a large
number of hypothesis tests,

217
00:11:46.270 --> 00:11:50.360
controlling the probability of even one
false positive might be pretty extreme.

218
00:11:50.360 --> 00:11:53.580
You might want to allow for a few pos,
false positives if

219
00:11:53.580 --> 00:11:56.120
that will allow you to discover a lot of
more real signals.

220
00:11:57.730 --> 00:12:00.320
So that's where the false discovery rate
comes in.

221
00:12:00.320 --> 00:12:02.910
It's probably the most popular for error
rate or

222
00:12:02.910 --> 00:12:06.890
multiple testing correction when
performing very many hypothesis tests.

223
00:12:06.890 --> 00:12:09.550
These examples, like I said, came up in
genomics.

224
00:12:09.550 --> 00:12:13.490
Imaging, astronomy or other signal
processing disciplines in particular.

225
00:12:13.490 --> 00:12:15.580
But it also comes up in a lot of other
different places.

226
00:12:15.580 --> 00:12:19.050
So the basic idea here is suppose you do m
hypothesis

227
00:12:19.050 --> 00:12:22.660
tests, and you want to control the false
discovery rate at level alpha.

228
00:12:22.660 --> 00:12:25.660
So the expected number of false
discoveries divided

229
00:12:25.660 --> 00:12:27.730
by the total number of discoveries is
controlled.

230
00:12:28.740 --> 00:12:31.220
You can think of this sort of as that
level of noise in the results.

231
00:12:31.220 --> 00:12:34.760
So, if you have F, F, FDR of Alpha, you
expect to

232
00:12:34.760 --> 00:12:38.690
have Alpha Percent of the things that you
are claiming to be false.

233
00:12:39.690 --> 00:12:42.020
So, what you do is you just calculate the
P values

234
00:12:42.020 --> 00:12:45.310
normally and then you order the P values
from smallest to largest.

235
00:12:45.310 --> 00:12:48.010
And the way that we denote that is in
parenthesis,

236
00:12:48.010 --> 00:12:51.070
we put the number that represents the
order of the P-Value.

237
00:12:51.070 --> 00:12:53.690
So this is no longer the P value that we
calculated.

238
00:12:53.690 --> 00:12:56.530
It's now the smallest P value that we
calculate.

239
00:12:56.530 --> 00:12:59.300
And then we order them all the way up to
the mth P

240
00:12:59.300 --> 00:13:03.740
value, so there are m hypothesis tests, so
this is the maximum P value.

241
00:13:04.810 --> 00:13:07.100
Then you go through and for the ith order
P

242
00:13:07.100 --> 00:13:09.630
value, you look to see whether there's
less than, or

243
00:13:09.630 --> 00:13:12.510
equal to alpha times i divided by m, and
if

244
00:13:12.510 --> 00:13:15.490
it's true, then you call it significant
and if not.

245
00:13:15.490 --> 00:13:18.350
You don't, this then is procedure is

246
00:13:18.350 --> 00:13:21.400
designed to control this false discovery
rate here.

247
00:13:22.640 --> 00:13:23.860
The pros are that it's still pretty

248
00:13:23.860 --> 00:13:26.600
easy to calculate like the Brom Brody
correction.

249
00:13:26.600 --> 00:13:29.970
It's less conservative and maybe much less
conservative if there's a

250
00:13:29.970 --> 00:13:33.070
lot of signal and you allow for just a few
false positives.

251
00:13:33.070 --> 00:13:36.220
You might be able to find a lot more of
the real signals.

252
00:13:37.280 --> 00:13:39.410
The cons are it does allow for more false
positives,

253
00:13:39.410 --> 00:13:42.270
so if you let the error rate be very
large,

254
00:13:42.270 --> 00:13:44.170
you might find a lot of false positives
among the

255
00:13:44.170 --> 00:13:47.880
significant results, and it might also
behave strangely under dependence.

256
00:13:47.880 --> 00:13:49.100
In other words.

257
00:13:49.100 --> 00:13:51.460
If you perform hypothesis tests that are
related to each

258
00:13:51.460 --> 00:13:55.550
other, say for example including different
sets of parameters in the

259
00:13:55.550 --> 00:13:58.240
same regression model and trying out a
bunch of different regression

260
00:13:58.240 --> 00:14:01.250
models you can get strange behavior of the
false discovery rate.

261
00:14:02.530 --> 00:14:03.400
So, here I'm going to show you an

262
00:14:03.400 --> 00:14:09.000
example of how these significance
calculations are performed.

263
00:14:09.000 --> 00:14:12.940
Basically how the hypothesis tests are
performed for the different corrections.

264
00:14:12.940 --> 00:14:16.220
And so I'm going to do this example of
with P-values.

265
00:14:16.220 --> 00:14:18.900
And I'm going to control all of the error
rates

266
00:14:18.900 --> 00:14:22.144
that we're going to be talking about at
the level 0.2.

267
00:14:23.170 --> 00:14:25.350
So, here are the ten P-Values.

268
00:14:25.350 --> 00:14:27.560
They're ordered by how small they are.

269
00:14:27.560 --> 00:14:31.850
So this is the smallest P-Value up to the
tenth P-Value, which is the largest one.

270
00:14:31.850 --> 00:14:34.540
And on the Y-Axis is the P-Value itself.

271
00:14:34.540 --> 00:14:37.240
And so the red line represents all of the
P-Values that we would

272
00:14:37.240 --> 00:14:42.070
call signficant at L Alpha equals 0.2, if
we did no correction at all.

273
00:14:42.070 --> 00:14:44.950
So it's basically just all the P-Values
less than 0.2.

274
00:14:44.950 --> 00:14:48.920
And so this, there's no correction
approach actually controls.

275
00:14:48.920 --> 00:14:52.300
The false positive rate, but remember that
can lead to a

276
00:14:52.300 --> 00:14:56.150
large number of false positives if you're
performing many hypothesis test.

277
00:14:57.340 --> 00:15:00.690
So the next thing to look at is the false
discovery rate.

278
00:15:00.690 --> 00:15:05.810
So this is controlling the proportion of
false positives at level of 0.2.

279
00:15:05.810 --> 00:15:11.521
In other words, we expect about 0.2% or
sorry, about 20%

280
00:15:11.521 --> 00:15:14.436
of all the results we call significant to
be false positives.

281
00:15:14.436 --> 00:15:23.230
And so the way that this is calculated is
actually following this gray line.

282
00:15:23.230 --> 00:15:25.400
So, we're going to order the P-Values from

283
00:15:25.400 --> 00:15:27.420
smallest to largest and each time we're
going to

284
00:15:27.420 --> 00:15:31.590
compare it to a a linear line where

285
00:15:31.590 --> 00:15:35.530
the slope is determined by this alpha
level here.

286
00:15:35.530 --> 00:15:39.280
And so, we actually would just call this
first three P-Values significant.

287
00:15:39.280 --> 00:15:42.270
So we find three significant P-Values.

288
00:15:42.270 --> 00:15:44.276
And we're controlling a slightly different
error measure.

289
00:15:44.276 --> 00:15:48.769
Finally the Bonferroni correction, down
here is actually just taking 0.2

290
00:15:48.769 --> 00:15:52.570
and dividing by 10, the number of
hypothesis that we're testing.

291
00:15:52.570 --> 00:15:56.260
So in this case it's 0.02, is this line
straight across here.

292
00:15:56.260 --> 00:15:58.600
And in this case we'd only dis, we've only

293
00:15:58.600 --> 00:16:02.460
discovered these two or called these two
P-Values significant.

294
00:16:02.460 --> 00:16:04.900
And all the rest we would call
insignificant.

295
00:16:04.900 --> 00:16:05.580
Not significant.

296
00:16:05.580 --> 00:16:08.510
But the Bonferroni correction here is
controlling

297
00:16:08.510 --> 00:16:11.310
this much more stringent family wise error
rate.

298
00:16:11.310 --> 00:16:12.800
So this hopefully shows you a little bit

299
00:16:12.800 --> 00:16:14.500
about how the different procedures work,
sort of

300
00:16:14.500 --> 00:16:16.700
conceptually in terms of where the
cut-offs are

301
00:16:16.700 --> 00:16:18.779
drawn for different sorts of sets of
P-Values.

302
00:16:20.550 --> 00:16:25.220
Another approach is to adjust the P-values
rather than to adjust the alpha level.

303
00:16:25.220 --> 00:16:28.750
And so in this case, we're going

304
00:16:28.750 --> 00:16:31.910
to calculate it's, what's called adjusted
P-values.

305
00:16:31.910 --> 00:16:35.320
This is the, the reason why I bring up
this approach is because it's easy.

306
00:16:35.320 --> 00:16:38.780
And sort of a direct calculation is
available in R.

307
00:16:38.780 --> 00:16:40.340
Something to keep in mind is that once
you've

308
00:16:40.340 --> 00:16:43.950
adjusted P-values, they are no longer
classically defined P-values.

309
00:16:43.950 --> 00:16:45.660
In other words they don't have the same
properties

310
00:16:45.660 --> 00:16:48.340
of classically defined P-values, and
shouldn't be treated that way.

311
00:16:48.340 --> 00:16:50.670
But they can be used to control error

312
00:16:50.670 --> 00:16:53.495
measures directly without adjusting the
alpha parameter [INAUDIBLE].

313
00:16:54.710 --> 00:16:58.020
So, here's an example of how this might
work for the Bonferroni correction.

314
00:16:58.020 --> 00:17:00.840
So, suppose we have these m P-values.

315
00:17:00.840 --> 00:17:04.188
One thing that we could do is we could
adjust them by

316
00:17:04.188 --> 00:17:10.230
calculating m times each P-value, and
taking the max of that and one.

317
00:17:10.230 --> 00:17:12.200
So in other words the P-values can't be,

318
00:17:12.200 --> 00:17:14.180
the adjusted P-values can't be larger than
one.

319
00:17:14.180 --> 00:17:16.980
Just like the P-value themselves can't be
larger than one.

320
00:17:16.980 --> 00:17:19.580
But we have multiplied every P-value by m.

321
00:17:19.580 --> 00:17:22.822
So remember for the Bonferroni correction,
we

322
00:17:22.822 --> 00:17:25.640
were going to divide the alpha level by m.

323
00:17:25.640 --> 00:17:27.685
So if instead, we multiply the p value by

324
00:17:27.685 --> 00:17:30.810
m, we can just calculate the number of
times our

325
00:17:30.810 --> 00:17:32.950
new P-values are less than alpha and it
will give

326
00:17:32.950 --> 00:17:36.980
you the exact same set of results that are
significant.

327
00:17:36.980 --> 00:17:37.830
So in other words we can use

328
00:17:37.830 --> 00:17:42.660
these, familiarized error rate or
Bonferroni adjusted P-values.

329
00:17:42.660 --> 00:17:45.130
To calculate significance by comparing it
to the original

330
00:17:45.130 --> 00:17:48.060
alpha level that we might have been
interested in.

331
00:17:48.060 --> 00:17:48.820
In this case say, 0.05.

332
00:17:48.820 --> 00:17:53.530
So if we multiply the P-values times the
number of tests performed.

333
00:17:53.530 --> 00:17:55.890
And look at how many are less than alpha
then

334
00:17:55.890 --> 00:17:58.610
we will control the familiarized error
rate at level alpha.

335
00:18:00.500 --> 00:18:02.920
So here's an example with no true
positives.

336
00:18:02.920 --> 00:18:08.200
So in this case, I've simulated a bunch of
data sets, so a 1,000 data sets.

337
00:18:08.200 --> 00:18:10.410
In each case, I generate a normal y and

338
00:18:10.410 --> 00:18:12.650
a normal x that have no relationship to
each other.

339
00:18:13.700 --> 00:18:18.560
And then, I fiddled in your model relating
those to variables Y to X and I get

340
00:18:18.560 --> 00:18:22.570
the coefficients of that I, I take the
summary

341
00:18:22.570 --> 00:18:25.530
of that linear model and get the
coefficient matrix.

342
00:18:25.530 --> 00:18:28.790
In that coefficient matrix in the second
row.

343
00:18:28.790 --> 00:18:33.060
The fourth columns the P-value for the
relationship between y and x.

344
00:18:33.060 --> 00:18:37.890
So I calculate the P-value for all 1000
different simulated examples.

345
00:18:37.890 --> 00:18:40.670
And then I look at the number of P-values
less than 0.05.

346
00:18:40.670 --> 00:18:42.560
So remember in none of these cases was

347
00:18:42.560 --> 00:18:45.340
there actually a relationship between the
two variables.

348
00:18:45.340 --> 00:18:48.570
But still we get 51 or about 5% of the
tests being

349
00:18:50.690 --> 00:18:52.120
performed or called significant.

350
00:18:52.120 --> 00:18:53.950
Even though there's no relationship.

351
00:18:55.070 --> 00:18:58.460
So what happens if I, adjust the P-values
and apply

352
00:18:58.460 --> 00:19:01.950
the false famiarlize wise error rate or
the false discovery rate.

353
00:19:01.950 --> 00:19:03.740
So, for example.

354
00:19:03.740 --> 00:19:08.745
What I can do is use the P.adjust function
in R, so I just say I apply

355
00:19:08.745 --> 00:19:14.430
P.adjust to the P-value vector that I
calculated.

356
00:19:14.430 --> 00:19:18.100
So this P-value vector has all the
P-values from all 1000 different studies.

357
00:19:19.295 --> 00:19:21.260
P.adjust gets applied to that.

358
00:19:21.260 --> 00:19:22.580
In the first case I say method equals

359
00:19:22.580 --> 00:19:25.483
Bonferroni because I want to do the
Bonferroni correction.

360
00:19:25.483 --> 00:19:27.480
And again, now that I've corrected the
P-values I

361
00:19:27.480 --> 00:19:30.070
can just compare them to a standard alpha
level, in

362
00:19:30.070 --> 00:19:32.640
this case 0.05 and I look at how many are

363
00:19:32.640 --> 00:19:34.700
less than that and in this case there are
zero.

364
00:19:34.700 --> 00:19:40.480
So, when there are no true positives we
find very few true significant results.

365
00:19:40.480 --> 00:19:41.900
When we're controlling the famiarlize
area.

366
00:19:41.900 --> 00:19:43.040
Which is good, we shouldn't find

367
00:19:43.040 --> 00:19:45.320
any significant results because there's no
relationship.

368
00:19:46.430 --> 00:19:50.260
Similarly, we can do the same thing, but
instead of controlling

369
00:19:50.260 --> 00:19:51.850
using the Bonferroni Correction, we can

370
00:19:51.850 --> 00:19:54.640
use what's called the Benjamini-Hochberg
Correction.

371
00:19:54.640 --> 00:19:56.220
Which is that correction I just talked
about

372
00:19:56.220 --> 00:19:59.160
a minute ago for controlling the false
discovery rate.

373
00:19:59.160 --> 00:20:03.000
So I can again adjust the P-values and
then look at the numbers that are

374
00:20:03.000 --> 00:20:04.420
less than 0.05, in this case again

375
00:20:04.420 --> 00:20:07.210
we don't discover anything which is good
because.

376
00:20:07.210 --> 00:20:09.080
There shouldn't be any discoveries in

377
00:20:09.080 --> 00:20:12.630
the case that there's no significant
relationships.

378
00:20:12.630 --> 00:20:16.020
So, I'm going to show another simple
simulated

379
00:20:16.020 --> 00:20:19.070
scenario, and so in this scenario I'm
going to

380
00:20:19.070 --> 00:20:23.720
have 50% of the time this is going to be a
relationship between the two variables.

381
00:20:23.720 --> 00:20:25.780
So I'm going to simulate again one
thousand

382
00:20:25.780 --> 00:20:29.190
different y and x variables for the first.

383
00:20:30.710 --> 00:20:31.210
500

384
00:20:32.850 --> 00:20:33.962
sets of variables.

385
00:20:33.962 --> 00:20:37.730
I'm going to generate, a y value that's
independent of x.

386
00:20:37.730 --> 00:20:40.900
For the last 500, I'm going to generate a
y value that has a

387
00:20:40.900 --> 00:20:44.420
mean that's equal to 2 times x so there's
a relationship between y and x.

388
00:20:44.420 --> 00:20:48.720
So the first 500 beta is equal to zero and
the last 500 beta is equal to two.

389
00:20:48.720 --> 00:20:52.590
Again I calculate a P-value for each of
the cases.

390
00:20:52.590 --> 00:20:55.617
And then I can define the true status to

391
00:20:55.617 --> 00:20:58.810
be beta is equal to zero for the first
500.

392
00:20:58.810 --> 00:21:02.420
And data is equal not, is not equal to
zero for the last 500.

393
00:21:02.420 --> 00:21:06.330
Just so I can make a table and show what
the results are from this analysis.

394
00:21:07.690 --> 00:21:09.605
So if we look at the number of P-values
that is

395
00:21:09.605 --> 00:21:13.880
less than 0.05 by the true status this is
with no correction.

396
00:21:13.880 --> 00:21:19.450
I see that for the case where the, there
is actually no relationship between

397
00:21:19.450 --> 00:21:25.230
the two variables, I get again about 5% of
the time a false positive result.

398
00:21:25.230 --> 00:21:27.320
And then in this case the signal's very
strong.

399
00:21:27.320 --> 00:21:31.740
So I actually find that all of theuh,
P-values for

400
00:21:31.740 --> 00:21:34.350
the cases where there is a relationship
are less than 0.05.

401
00:21:34.350 --> 00:21:38.460
So I actually discover all of the, the
real signals that exist in this data set.

402
00:21:40.430 --> 00:21:44.080
So if I use a familiarlize error rate, I,
again, adjust

403
00:21:44.080 --> 00:21:49.120
now the P-values using p.adjust, apply to
the p value vector.

404
00:21:49.120 --> 00:21:51.520
I set the method to be Bonferroni,
calculate

405
00:21:51.520 --> 00:21:54.600
the number of times it's less than 0.05.

406
00:21:54.600 --> 00:22:00.790
In this case, I actually discover slightly
fewer, significant results.

407
00:22:00.790 --> 00:22:04.440
In other words I missed 23 of the cases
where there should be a signal.

408
00:22:04.440 --> 00:22:06.440
But now I actually have no false
positives.

409
00:22:06.440 --> 00:22:09.240
And that's because I'm again controlling
the probability of even

410
00:22:09.240 --> 00:22:13.350
one false positive to be less than 0.05 in
this case.

411
00:22:13.350 --> 00:22:16.680
In the case of the false discovery rate I
set the method to be equal to bh.

412
00:22:17.850 --> 00:22:20.650
And what I discovered is that here I, I

413
00:22:20.650 --> 00:22:24.150
do actually discovered all of this
significant results, but

414
00:22:24.150 --> 00:22:28.150
I discover actually fewer false positive
results than I

415
00:22:28.150 --> 00:22:31.840
would've discovered with out any kind of
multiple testing correction.

416
00:22:31.840 --> 00:22:34.780
And in this case, actually about 5% of the

417
00:22:36.570 --> 00:22:40.348
cases were actually called there to be a
true relationship.

418
00:22:40.348 --> 00:22:44.970
Only about 5% of the time are the is there
not actually a true relationship.

419
00:22:44.970 --> 00:22:49.320
So, in this case, I'm looking at the
percentage of the times that

420
00:22:49.320 --> 00:22:52.610
they're actually equal to zero but we
claim that it's not equal to zero.

421
00:22:54.920 --> 00:22:58.070
So the other thing that you can do, all of
this is not necessarily useful

422
00:22:58.070 --> 00:23:00.210
for performing the hypothesis test, it's
useful

423
00:23:00.210 --> 00:23:03.270
for kind of understanding what P
adjustment does.

424
00:23:03.270 --> 00:23:06.590
So I can plot the P values versus the
adjusted P values corrected

425
00:23:06.590 --> 00:23:08.190
for both the Bonfferoni method and the

426
00:23:08.190 --> 00:23:11.600
Benjamin Hochberger, in the case of
Bonferoni.

427
00:23:11.600 --> 00:23:15.540
What I'm doing is I'm just taking each P
value and multiplying it by

428
00:23:15.540 --> 00:23:18.740
the number of tests that I perform, so in
this case, multiplying it by 1,000.

429
00:23:18.740 --> 00:23:22.560
So, you can see the various smallest P
values are still less than one.

430
00:23:22.560 --> 00:23:25.830
But then, after I get to this point here,
all the P

431
00:23:25.830 --> 00:23:30.100
values multiplied by 1,000 are equal to
one or greater and so.

432
00:23:30.100 --> 00:23:31.740
Since I don't allow the adjusted p-values
to be

433
00:23:31.740 --> 00:23:35.250
greater than one, you just get a flat
line.

434
00:23:35.250 --> 00:23:37.150
On the other hand with the benjamini
Hochberger probe,

435
00:23:37.150 --> 00:23:39.710
you actually see this sort of increasing
function, so

436
00:23:39.710 --> 00:23:44.010
the P value is on the x axis and the
adjsusted P value is on the y axis.

437
00:23:44.010 --> 00:23:46.150
And you see that the adjusted P value is
slightly

438
00:23:46.150 --> 00:23:50.310
larger across the entire range than the
actual P value itself.

439
00:23:50.310 --> 00:23:54.060
But not dramatically larger, actually, for
this particular case.

440
00:23:54.060 --> 00:23:56.000
Because you actually have a lot of
significant results.

441
00:23:57.040 --> 00:23:58.940
So it's the notes and further resources.

442
00:23:58.940 --> 00:24:02.140
So first the notes, multiple testing is
actually an entire sub field

443
00:24:02.140 --> 00:24:05.600
and there's a whole bunch of different
corrections that you can possibly apply.

444
00:24:05.600 --> 00:24:07.950
Depending on the different dependant
structures and all the different

445
00:24:07.950 --> 00:24:11.250
sort of choices that you made in the
statistical model and.

446
00:24:11.250 --> 00:24:13.190
So depending on your problem you might
want to do a little.

447
00:24:13.190 --> 00:24:16.500
Further research on what's right
correction apply.

448
00:24:16.500 --> 00:24:20.220
The basic Bonferroni or Benjamini Hochberg
Correction is usually enough for most sort

449
00:24:20.220 --> 00:24:23.850
of standard problems but there's actually
if

450
00:24:23.850 --> 00:24:25.470
there's strong dependence between the test
for

451
00:24:25.470 --> 00:24:30.770
example you might want to consider method
equals BY in the p data chest function or

452
00:24:30.770 --> 00:24:36.600
looking to more direct adjustments for the
dependents between the hypothesis tests.

453
00:24:36.600 --> 00:24:39.560
This is actually an area in which I've
done a little bit of research,

454
00:24:39.560 --> 00:24:44.160
so hopefully you can take a look at some
of my papers on the area.

455
00:24:44.160 --> 00:24:48.140
So, for the resources are this is actually
quite a nice paper.

456
00:24:48.140 --> 00:24:53.550
A gentle introduction to multiple testing
procedures with applications in genomics.

457
00:24:53.550 --> 00:24:55.580
Which is an area I work in and where
multiple

458
00:24:55.580 --> 00:24:59.280
testing has really flourished in terms of
a statistical discipline.

459
00:24:59.280 --> 00:25:01.610
Similarly, the statistical significance
for genome wide

460
00:25:01.610 --> 00:25:04.270
studies is a very nice gentle
introduction.

461
00:25:04.270 --> 00:25:07.282
Introductions in terms of a paper to read.

462
00:25:07.282 --> 00:25:10.320
again, it's focused on molecular biology
but it's pretty easy to read.

463
00:25:10.320 --> 00:25:12.680
I think even if you're not an expert in
that area.

464
00:25:12.680 --> 00:25:14.820
And finally this is a very nice sort of

465
00:25:14.820 --> 00:25:18.330
introduction to multiple testing that goes
over to basics.

466
00:25:18.330 --> 00:25:19.590
A lot of what I've covered now but may

467
00:25:19.590 --> 00:25:21.810
be a little bit more depth in case you're
interested.