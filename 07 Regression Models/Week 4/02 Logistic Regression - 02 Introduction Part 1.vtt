WEBVTT

1
00:00:00.370 --> 00:00:05.480
Hi, and welcome to the lecture on
generalized linear models for binary data.

2
00:00:05.480 --> 00:00:09.940
In this lecture, we're going to focus on
instances where the outcome is a zero,

3
00:00:09.940 --> 00:00:11.160
one random variable.

4
00:00:11.160 --> 00:00:12.330
So it can only take two levels.

5
00:00:12.330 --> 00:00:17.870
And this occurs very
frequently in most analyses.

6
00:00:17.870 --> 00:00:20.270
For example,
in biostatistics we're all for

7
00:00:20.270 --> 00:00:23.490
doing things like survival analysis
where people are either alive or

8
00:00:23.490 --> 00:00:27.090
dead at the end of the study,
for example, a cancer study.

9
00:00:27.090 --> 00:00:31.850
We can look at wins versus losses, like
we're going to do today, or success or

10
00:00:31.850 --> 00:00:33.500
failures of any kind.

11
00:00:33.500 --> 00:00:34.910
And our model is going to be,

12
00:00:34.910 --> 00:00:39.350
we're going to model the data as if it's
a bunch of coin flips, where a success

13
00:00:39.350 --> 00:00:44.690
probability potentially depends
on a collection of covariates.

14
00:00:44.690 --> 00:00:47.210
Now if you have any
collection of zeros and

15
00:00:47.210 --> 00:00:51.900
ones where the success probability is
constant and they're independent, then

16
00:00:51.900 --> 00:00:56.460
the total number of successes or failures
is a so-called binomial random variable.

17
00:00:57.620 --> 00:01:04.220
Okay, so binomial random
variables are handled by binary

18
00:01:04.220 --> 00:01:09.770
logistic regression in the special case
where the covariate is just constant.

19
00:01:09.770 --> 00:01:12.380
Okay, so we're going to cover two cases,
binary and

20
00:01:12.380 --> 00:01:14.609
binomial, by covering the binary case.

21
00:01:17.020 --> 00:01:21.570
So, as a motivating example, let's look at
this data set that Jeff Leak organized.

22
00:01:22.730 --> 00:01:25.510
Jeff Leak is another one of
the instructors in our data science

23
00:01:25.510 --> 00:01:26.310
specialization.

24
00:01:26.310 --> 00:01:29.070
He teaches the practical machine learning,
getting and

25
00:01:29.070 --> 00:01:32.270
cleaning data and data scientist toolbox.

26
00:01:32.270 --> 00:01:34.970
So in this case,
you can get the data here, but

27
00:01:34.970 --> 00:01:37.370
it's also in the GitHub repository.

28
00:01:39.210 --> 00:01:44.680
The data set here has the Ravens'
win number at ones versus zeros,

29
00:01:44.680 --> 00:01:48.140
whether or not the Ravens won
coded as a factor variable,

30
00:01:48.140 --> 00:01:52.820
wins versus loss, the score,
Ravens' score, and the opponents score.

31
00:01:52.820 --> 00:01:55.330
Now on this case,
we're just going to look at

32
00:01:55.330 --> 00:01:59.560
to what extent the Ravens' score
predicts whether or not they won.

33
00:01:59.560 --> 00:02:04.690
Okay, so you can imagine, if you're in the
front office for the Ravens, who, I forgot

34
00:02:04.690 --> 00:02:09.960
to mention this, the Ravens are a U.S.
American football and

35
00:02:09.960 --> 00:02:15.110
a National Football League team and
based in Baltimore.

36
00:02:15.110 --> 00:02:17.870
So we all love the Ravens around here.

37
00:02:17.870 --> 00:02:22.240
So you can imagine, if you were in the
front office for the Ravens, wanting to

38
00:02:22.240 --> 00:02:26.490
predict wins with a lot of covariates,
score would be one of them, opponent's

39
00:02:26.490 --> 00:02:30.150
score would obviously be one of them, and
so on, but you'd want home versus away.

40
00:02:30.150 --> 00:02:33.810
And I think you can understand
the principles of binary

41
00:02:35.750 --> 00:02:40.430
logistic regression just by looking
at this example and, I hope,

42
00:02:40.430 --> 00:02:45.170
extending it, given everything that we've
learned about multivariate linear models

43
00:02:45.170 --> 00:02:49.530
will carry over very easily for you.

44
00:02:49.530 --> 00:02:51.140
So I just want to remind ourselves,

45
00:02:51.140 --> 00:02:54.450
we probably don't want to fit
this data with linear regression.

46
00:02:54.450 --> 00:03:00.200
If we were to model the probability
of a Ravens win, the event of

47
00:03:00.200 --> 00:03:05.090
a Ravens win one versus zero as a linear
progression we wouldn't be honoring,

48
00:03:05.090 --> 00:03:08.960
this equation here wouldn't
actually hold true for sure, right?

49
00:03:08.960 --> 00:03:14.550
Because usually we assume the errors
are Gaussian, just would not hold.

50
00:03:14.550 --> 00:03:19.140
Now that isn't to say that occasionally
fitting binary data with a linear model is

51
00:03:19.140 --> 00:03:21.060
the worst thing in the world to do.

52
00:03:21.060 --> 00:03:25.780
But it's usually only a quick first thing
that you do before you then move on to fit

53
00:03:25.780 --> 00:03:27.570
something like a logistic
regression model.

54
00:03:30.010 --> 00:03:34.300
So here we fit the linear regression in R,
and you get the output, and

55
00:03:34.300 --> 00:03:37.560
you see what the model
is trying to get at,

56
00:03:37.560 --> 00:03:41.570
the model is the slope, is trying to
get at probability of a Raven's win.

57
00:03:41.570 --> 00:03:44.170
However, you can get slope
estimates that are negative or

58
00:03:44.170 --> 00:03:47.620
above one if you fit
a linear regression model.

59
00:03:47.620 --> 00:03:49.925
Again, it's not the worst
thing in the world to do.

60
00:03:49.925 --> 00:03:54.500
Sometimes, if you're doing something
quick, you have to do it, but

61
00:03:54.500 --> 00:03:56.579
probably preferable would
be to model the odds.

62
00:03:57.960 --> 00:04:02.470
So just to remind ourselves what the odds
are, if we have a binary outcome,

63
00:04:02.470 --> 00:04:03.940
which in this case is whether or

64
00:04:03.940 --> 00:04:08.380
not the Ravens win, we want to model
the probability that the Ravens win.

65
00:04:08.380 --> 00:04:09.780
So I'm going to get me pen here.

66
00:04:13.462 --> 00:04:16.208
The probability that the Ravens win, but

67
00:04:16.208 --> 00:04:19.830
we're going to model it like
it's a bunch of coin flips.

68
00:04:19.830 --> 00:04:24.330
However, the success probability
will differ from game

69
00:04:24.330 --> 00:04:29.010
to game depending on how many
points the Ravens score, okay?

70
00:04:29.010 --> 00:04:32.839
So, that's the probability we
want to model so, the odds.

71
00:04:33.840 --> 00:04:37.580
The odds are defined as the probability
over one minus the probability, and

72
00:04:37.580 --> 00:04:38.940
in a couple of slides,

73
00:04:38.940 --> 00:04:43.990
I'll describe more about the history of
the odds and how people think about it.

74
00:04:43.990 --> 00:04:49.860
You can go backwards, by the way,
from the odds to the probability.

75
00:04:49.860 --> 00:04:55.440
In that the probability is equal
to the odds over 1 plus the odds.

76
00:04:55.440 --> 00:04:59.980
Okay, you go forwards from the probability
to the odds by taking the probability over

77
00:04:59.980 --> 00:05:01.370
1 minus the probability.

78
00:05:01.370 --> 00:05:04.290
You can go backwards from
the odds to get the probability

79
00:05:04.290 --> 00:05:06.160
by taking the odds over 1 plus the odds.

80
00:05:06.160 --> 00:05:07.430
So the two things are exchangeable.

81
00:05:09.880 --> 00:05:14.570
The log odds then is
just the log of the odds.

82
00:05:14.570 --> 00:05:19.990
And we use that so commonly that we gave
it a name and it's called the logit.

83
00:05:19.990 --> 00:05:23.220
So the logit is simply
the log of the odds.

84
00:05:23.220 --> 00:05:26.629
So what we're going to do is put
the model on the log of the odds.

85
00:05:32.320 --> 00:05:36.759
So, I think we've talked a lot about
linear versus logistic regression,

86
00:05:36.759 --> 00:05:41.057
both in the last lecture and in this
lecture, describing what are some of

87
00:05:41.057 --> 00:05:46.920
the considerations for why we want to
probably do this with logistic regression.

88
00:05:46.920 --> 00:05:50.320
But the main thing is that the model
assumptions clearly don't hold for

89
00:05:50.320 --> 00:05:51.010
linear regression.

90
00:05:51.010 --> 00:05:55.760
They're obviously, the inferences
probably are not anywhere near correct.

91
00:05:57.220 --> 00:06:00.600
Or we have no faith in them.

92
00:06:01.720 --> 00:06:09.130
But let's just draw the connection
between the two so that we can relate

93
00:06:09.130 --> 00:06:13.310
what we're doing in logistic regression
back to what we did in linear regression.

94
00:06:13.310 --> 00:06:17.880
So for linear regression we built
models like this, where the outcome was

95
00:06:17.880 --> 00:06:22.390
the intercept plus a slope
parameter times the predictor

96
00:06:22.390 --> 00:06:26.880
plus an error term, which is a nice way
to write out the model and convenient.

97
00:06:26.880 --> 00:06:30.720
But another way to right out this same
model is just to say that the expected

98
00:06:30.720 --> 00:06:34.168
value of the outcome was
this linear relationship.

99
00:06:34.168 --> 00:06:39.494
So the expected value of the outcome,

100
00:06:39.494 --> 00:06:44.488
in this case, the expected value is

101
00:06:44.488 --> 00:06:49.158
of a zero, one random variable.

102
00:06:49.158 --> 00:06:50.800
The expected value of a zero,
one random variable is the probability.

103
00:06:50.800 --> 00:06:54.470
So the expected value
of a fair coin is 0.5.

104
00:06:54.470 --> 00:06:59.270
So instead of doing what
we do in linear regression,

105
00:06:59.270 --> 00:07:03.130
which is to model the expected value
of the outcome directly on that scale,

106
00:07:03.130 --> 00:07:06.540
why don't we model the expected
value of the outcome,

107
00:07:06.540 --> 00:07:11.000
which is the probability, why don't we
model that with something like this,

108
00:07:13.600 --> 00:07:17.983
e to the linear regression over
1 + e to the linear regression.

109
00:07:17.983 --> 00:07:22.474
Well this, right here,
this equation, if we invert it and

110
00:07:22.474 --> 00:07:27.790
if we work with it a little bit, we

111
00:07:27.790 --> 00:07:32.520
get that it says that the log of the odds
is the linear regression relationship.

112
00:07:32.520 --> 00:07:37.440
So what we're doing in binary
generalized linear models is,

113
00:07:37.440 --> 00:07:43.380
in this particular logistic regression,
we're modeling our data

114
00:07:43.380 --> 00:07:48.760
as if it was a bunch of coin flips where
the success probability changes with i.

115
00:07:50.120 --> 00:07:55.130
And we're relating the coin,
that success probability,

116
00:07:55.130 --> 00:08:02.150
to our regressors by taking the log of
odds associated with that probability.

117
00:08:02.150 --> 00:08:03.570
So you can go backwards.

118
00:08:03.570 --> 00:08:06.050
This is the log of the odds, okay?

119
00:08:06.050 --> 00:08:11.080
You could go backwards to the probability
using this equation up here.

120
00:08:11.080 --> 00:08:14.460
And I like to call e to
the a over 1 + e to the a,

121
00:08:17.300 --> 00:08:22.990
this function that inverts the log
of the odds back to the probability,

122
00:08:22.990 --> 00:08:24.780
I like to call it the xbit.

123
00:08:24.780 --> 00:08:27.650
So the log of the odds
is called the logit.

124
00:08:27.650 --> 00:08:30.140
I like to call the function
that goes backwards the xbit.

125
00:08:31.710 --> 00:08:33.590
Okay, so that's all we're doing.

126
00:08:33.590 --> 00:08:34.210
That's all we're doing.

127
00:08:34.210 --> 00:08:36.310
But that's the clever bit of
generalizing your models.

128
00:08:36.310 --> 00:08:40.520
We're not actually putting
the model on the scale.

129
00:08:40.520 --> 00:08:45.740
We're not actually putting
the linear regressions parameters

130
00:08:45.740 --> 00:08:50.770
defining inequality between the outcomes
and the linear regression parameters.

131
00:08:50.770 --> 00:08:55.670
Instead we're saying that the outcome
depends on a probability distribution.

132
00:08:55.670 --> 00:08:59.290
That probability distribution
depends on the success probability.

133
00:08:59.290 --> 00:09:02.850
That success probability then
depends on the regressors.

134
00:09:03.880 --> 00:09:04.380
Okay?

135
00:09:11.629 --> 00:09:16.693
So interpreting logistic regression
is pretty straight forward,

136
00:09:16.693 --> 00:09:21.040
and I should mention all the logs
that we do in these notes,

137
00:09:21.040 --> 00:09:26.220
these are natural log, l-o-g,
meaning natural log in this case.

138
00:09:26.220 --> 00:09:28.990
Most mathematicians, when they
write l-o-g they mean natural log.

139
00:09:30.470 --> 00:09:35.770
So in this case what happens?

140
00:09:35.770 --> 00:09:43.230
So if I were to plug in a zero for
the Ravens score I would just get b0.

141
00:09:43.230 --> 00:09:50.680
So b0 is the log odds of a Ravens
win if they score 0 points.

142
00:09:50.680 --> 00:09:55.230
Then e to the b0 over 1 + e to

143
00:09:55.230 --> 00:09:59.760
the b0 would be the probability
that the Ravens won.

144
00:09:59.760 --> 00:10:03.700
That's the model based probability that
the Ravens win if they score zero points.

145
00:10:03.700 --> 00:10:09.230
Now in this case, this is maybe a not

146
00:10:09.230 --> 00:10:14.140
perfect component of this model, because
it gets estimated to be non-zero, and

147
00:10:14.140 --> 00:10:16.620
we know that the team can't
win if they score zero points.

148
00:10:16.620 --> 00:10:18.530
They could tie, but they can't win.

149
00:10:22.260 --> 00:10:26.836
So b1, okay, so remember, let me get,

150
00:10:28.879 --> 00:10:33.170
Remember how we interpreted our
slope coefficient from before.

151
00:10:33.170 --> 00:10:36.954
We have b0 + b1 and

152
00:10:36.954 --> 00:10:41.406
let me put in the (RSi +

153
00:10:41.406 --> 00:10:46.530
1)- b0 + b1 (RSi).

154
00:10:48.080 --> 00:10:50.700
Okay, then let me put
brackets around that.

155
00:10:50.700 --> 00:10:56.987
Okay, then that equation, if you work it
out, that equation works out be b1, okay?

156
00:10:58.800 --> 00:10:59.649
So what is this?

157
00:11:00.650 --> 00:11:07.124
This is the logit of the probability
that the Ravens win given the score + 1,

158
00:11:07.124 --> 00:11:14.550
and this is the logit of the probability
the Ravens win given the score itself.

159
00:11:14.550 --> 00:11:15.450
So what is b1?

160
00:11:15.450 --> 00:11:21.020
It's the increase or decrease depending
on whether it's positive or negative

161
00:11:21.020 --> 00:11:27.290
in the log odds of the probability that
the Ravens win associated with a one unit

162
00:11:27.290 --> 00:11:31.600
increase in the regression variable, or in
this case, a one point increase in score.

163
00:11:32.710 --> 00:11:35.900
Okay?
And if we exponetiate this,

164
00:11:35.900 --> 00:11:39.250
remember that if this is
a difference on the log scale, so

165
00:11:39.250 --> 00:11:44.180
if we exponentate it,
we get a ratio back on the original scale.

166
00:11:44.180 --> 00:11:51.030
So if we exponentiate this,
we get that e to the b1 is then the ratio

167
00:11:51.030 --> 00:11:57.880
of the odds comparing a one
point increase in score,

168
00:11:57.880 --> 00:12:02.681
the ratio of the odds of the Ravens
winning for one point score increase.

169
00:12:02.681 --> 00:12:04.673
Okay?

170
00:12:04.673 --> 00:12:12.250
And we don't have multiple regressors like
we studied a lot in linear regression.

171
00:12:12.250 --> 00:12:13.130
We don't have that for

172
00:12:13.130 --> 00:12:17.890
this particular example, but
the extension is pretty direct.

173
00:12:17.890 --> 00:12:22.110
Everything, just like in linear
regression, multi variable regression,

174
00:12:22.110 --> 00:12:26.780
everything is just interpreted presuming
the other regressors are held fixed.

175
00:12:26.780 --> 00:12:31.620
So remember our interpretation in
multivariable regression was that

176
00:12:33.070 --> 00:12:37.280
the increase associated with a one
unit change in the regressor,

177
00:12:37.280 --> 00:12:40.070
holding the other
regression variables fixed.

178
00:12:40.070 --> 00:12:46.390
That same interpretation carries over
in logistic regression, e to the b1.

179
00:12:46.390 --> 00:12:51.070
If we had a bunch of covariates
added in this, let's say whether or

180
00:12:51.070 --> 00:12:56.454
not it was a home or away game,
if we had that one extra covariates added,

181
00:12:56.454 --> 00:12:59.590
then e to the b1 would be the odds ratio

182
00:13:03.850 --> 00:13:08.710
of a Ravens win for a one unit increase
in the score holding whether or

183
00:13:08.710 --> 00:13:10.698
not it was an away or a home game fixed.

184
00:13:10.698 --> 00:13:14.839
Okay, so

185
00:13:14.839 --> 00:13:20.560
the logistic regression coefficients
are pretty easily interpretable.

186
00:13:20.560 --> 00:13:24.890
The log is a nice function
that allows us to sort of

187
00:13:24.890 --> 00:13:27.800
invert additive things and
turn them into ratios.

188
00:13:27.800 --> 00:13:31.290
And people like to talk in terms or
ratios, so

189
00:13:31.290 --> 00:13:32.860
it's a very nice interpretation.

190
00:13:33.860 --> 00:13:36.405
I want to talk a minute
about the history of odds.

191
00:13:36.405 --> 00:13:40.660
So it goes back to this
idea of a fair game.

192
00:13:40.660 --> 00:13:45.580
So imagine if you're playing a game
where if you flip a coin with success

193
00:13:45.580 --> 00:13:49.980
probability p, and
if it comes up heads you win X dollars.

194
00:13:49.980 --> 00:13:54.320
If it comes up tails, the person you're
playing against wins Y dollars, and

195
00:13:54.320 --> 00:13:56.130
you lose Y dollars.

196
00:13:56.130 --> 00:13:57.410
Okay?

197
00:13:57.410 --> 00:14:01.790
So what should we set X and
Y in order for the game to be fair?

198
00:14:01.790 --> 00:14:02.990
Okay.
In this case,

199
00:14:02.990 --> 00:14:05.830
the coin isn't necessarily a fair coin.

200
00:14:05.830 --> 00:14:08.160
p is not necessarily 0.5.

201
00:14:08.160 --> 00:14:10.900
So what is your expected earnings?

202
00:14:10.900 --> 00:14:14.210
Well if you took the inference class you
would you know that your expected earnings

203
00:14:14.210 --> 00:14:20.550
is how much you would win

204
00:14:20.550 --> 00:14:25.750
given the coin came up heads,
minus how much you would

205
00:14:25.750 --> 00:14:31.320
lose given the coin came up tails, okay?

206
00:14:31.320 --> 00:14:32.980
And if we want it to be a fair game,

207
00:14:32.980 --> 00:14:36.330
we want our expected earnings
to be equal to zero.

208
00:14:36.330 --> 00:14:41.120
Well if we solve that,
we get Y / X = p / 1- p.

209
00:14:41.120 --> 00:14:45.760
So, in other words,
the odds can be said as how much

210
00:14:45.760 --> 00:14:49.100
would you be willing to pay for
a p probability of winning a dollar.

211
00:14:51.760 --> 00:14:54.815
In other words, setting X equal to 1.

212
00:14:54.815 --> 00:15:01.230
And then if p is bigger than 0.5
you'll have to pay more than you lose,

213
00:15:01.230 --> 00:15:08.960
and p is less than 0.5 you'd have to pay
less than if you lose than if you win.

214
00:15:08.960 --> 00:15:15.920
And so this is why,
if you go to the horse track

215
00:15:15.920 --> 00:15:20.350
like Pimlico here in Baltimore, they'll
give you the odds against a horse winning.

216
00:15:20.350 --> 00:15:21.350
So they'll say 10 to 1.

217
00:15:21.350 --> 00:15:24.030
Why would they do that?

218
00:15:24.030 --> 00:15:28.380
Because, if you bet a dollar, that would
mean if the horse won you got $10.

219
00:15:28.380 --> 00:15:31.680
So you can do the calculation
very quickly in your head

220
00:15:31.680 --> 00:15:33.740
because they're giving you fair game.

221
00:15:35.190 --> 00:15:39.450
It's an interesting side note about
the horse betting analogy in that

222
00:15:39.450 --> 00:15:42.820
the way in which they set
the probabilities for

223
00:15:42.820 --> 00:15:47.850
horse betting is they do it in a way
adaptively as the bets come in.

224
00:15:47.850 --> 00:15:52.120
The more people bet on a horse
the probability that their

225
00:15:52.120 --> 00:15:55.130
odds that they say that
it's going to win goes up.

226
00:15:55.130 --> 00:15:58.780
The odds that they says it's going to
lose goes down as more people bet on it.

227
00:16:00.200 --> 00:16:04.590
And they do it that way, not because that
they think it's an accurate reflection of

228
00:16:04.590 --> 00:16:05.990
whether or not the horse will win, but

229
00:16:05.990 --> 00:16:08.890
just to balance the actual
money coming in and out.

230
00:16:08.890 --> 00:16:11.470
So that if that horse wins or loses,

231
00:16:11.470 --> 00:16:13.620
they know exactly how much
they would win or lose.

232
00:16:13.620 --> 00:16:15.630
And they can balance it out that way,

233
00:16:15.630 --> 00:16:19.090
so they're guaranteed to neither win nor
lose money.

234
00:16:21.070 --> 00:16:26.120
But what's interesting about that is
over a long period of looking at horses,

235
00:16:26.120 --> 00:16:31.630
that aggregate odds that
winds up from the betting,

236
00:16:31.630 --> 00:16:35.670
that winds up being a fairly accurate
representation of the percentage

237
00:16:35.670 --> 00:16:39.030
of times the horse wins,
which is interesting.

238
00:16:39.030 --> 00:16:43.880
That just says that people
are somehow wise as a collective, and

239
00:16:43.880 --> 00:16:46.830
they create an efficient, fair market for

240
00:16:46.830 --> 00:16:51.670
gambling by virtue of betting
correctly in aggregate.

241
00:16:53.350 --> 00:16:59.500
And then also you might wonder how
the casino or how the track makes money.

242
00:16:59.500 --> 00:17:03.480
They make money by taking a little
bit of a fee for every bet.

243
00:17:04.710 --> 00:17:10.030
So they are basically guaranteed to win,
so when the phrase,

244
00:17:10.030 --> 00:17:13.190
the house wins, is literally true.

245
00:17:13.190 --> 00:17:16.590
It means that the house is
basically guaranteed to win.

246
00:17:16.590 --> 00:17:20.180
They set up the odds so that,
whether the horse wins or

247
00:17:20.180 --> 00:17:25.560
loses, they're monetarily neutral, and
they take a little money off of each bet.

248
00:17:25.560 --> 00:17:26.410
That's an interesting fact.

249
00:17:26.410 --> 00:17:29.330
And of course, it's more complicated
than that because you've

250
00:17:29.330 --> 00:17:33.341
got a big complex organization,
thousands of people running it.

251
00:17:33.341 --> 00:17:37.580
So of course it's far more complicated
than that, but that's the gist of it.

252
00:17:38.960 --> 00:17:44.588
So if you want a good risk-free position,
become a bookie.

253
00:17:49.280 --> 00:17:53.936
So next up I'm going to show us how to
visualize fitting logistic regression

254
00:17:53.936 --> 00:17:56.800
curves using a computer experiment.

255
00:17:56.800 --> 00:17:59.160
So I look forward to seeing
you in that lecture.