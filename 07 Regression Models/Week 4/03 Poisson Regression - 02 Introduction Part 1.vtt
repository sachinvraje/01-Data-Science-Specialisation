WEBVTT

1
00:00:00.320 --> 00:00:02.830
Hi, and
welcome to our second to the last lecture.

2
00:00:02.830 --> 00:00:08.560
This lecture is on Poisson GLNs, and
I should give some credit to Jeff Leek

3
00:00:08.560 --> 00:00:13.320
who I got much of this content from,
from an earlier version of this class.

4
00:00:15.130 --> 00:00:23.540
So modeling count data arises
quite frequently in applications.

5
00:00:23.540 --> 00:00:27.940
For example, the number of calls to
a call center, the number of flu cases.

6
00:00:27.940 --> 00:00:32.130
And in each of these cases, the counts
are unbounded in the sense of well,

7
00:00:32.130 --> 00:00:34.470
there might be some
theoretical bounded account,

8
00:00:34.470 --> 00:00:37.140
the total number of people in the world or
whatever.

9
00:00:37.140 --> 00:00:41.280
However, we don't really know what that
is or that number is really large,

10
00:00:41.280 --> 00:00:45.430
relative to the count
that we're looking at.

11
00:00:45.430 --> 00:00:50.170
So in addition to count, data can come
in the form of rates or proportions,

12
00:00:50.170 --> 00:00:55.660
such as the percentage of people passing
a test, or in terms of rates, think

13
00:00:55.660 --> 00:01:01.640
about the number of cases or something
like that, that occur over a unit of time.

14
00:01:01.640 --> 00:01:08.137
My favorite example is from a nuclear pump
failure experiment where we're looking

15
00:01:08.137 --> 00:01:14.742
at the number of instances that nuclear
pumps failure, per, failed per unit time.

16
00:01:14.742 --> 00:01:15.880
So that would be a rate.

17
00:01:16.950 --> 00:01:20.510
A very common rate that occurs in
bio statistics and public health.

18
00:01:20.510 --> 00:01:23.310
Where I work in is, the so
called incidence rate,

19
00:01:23.310 --> 00:01:28.270
which is the number of newly developed
cases per person time at risk.

20
00:01:28.270 --> 00:01:32.470
Okay so
all of these are instances of counts and

21
00:01:32.470 --> 00:01:35.360
rates and proportions are also
you can think of as counts.

22
00:01:35.360 --> 00:01:37.540
Both because the numerator is a count and

23
00:01:37.540 --> 00:01:42.490
whatever you're dividing by either the
percent time at risk or the total time or

24
00:01:42.490 --> 00:01:48.470
the total sample or the number of
trials or something like that.

25
00:01:48.470 --> 00:01:53.390
That's a second number that we're going
to show you how to deal with as well when

26
00:01:53.390 --> 00:01:55.790
looking at the numerator,
the count part, okay?

27
00:01:55.790 --> 00:01:58.840
And all of these can be
handled with Poisson GLMs.

28
00:02:00.510 --> 00:02:05.419
So the Poisson Distribution is
a useful model for counts and rates.

29
00:02:07.360 --> 00:02:11.470
And again a rate is a count
per some monitoring time.

30
00:02:11.470 --> 00:02:15.740
So, often incidence rates and

31
00:02:18.420 --> 00:02:23.860
web traffic and all these other things
are modeled by Poisson distributions.

32
00:02:23.860 --> 00:02:28.170
A very common use of the Poisson
distribution is approximating binomial

33
00:02:28.170 --> 00:02:32.430
probabilities where the success
probability is very small and the end is

34
00:02:32.430 --> 00:02:36.290
very large, so you can think of that as an
instance of the sort of approximated and

35
00:02:36.290 --> 00:02:40.490
unbounded count,
even though the actual count is bounded.

36
00:02:42.340 --> 00:02:46.860
And then an application that I like quite
a bit is so called Contingency Table data.

37
00:02:46.860 --> 00:02:51.150
So if you have an instance where
you just counted the number of

38
00:02:53.460 --> 00:02:55.950
occurrences of a different
collection of variables.

39
00:02:55.950 --> 00:03:00.730
So if took a random sample of people and
I counted the number of people that

40
00:03:00.730 --> 00:03:05.500
had blonde hair, brown hair and
black hair and I cross tabulated

41
00:03:05.500 --> 00:03:10.780
that with the number of people who had
blue eyes, brown eyes and hazel eyes.

42
00:03:10.780 --> 00:03:15.490
Okay that table of counts is
called a contingency table and

43
00:03:15.490 --> 00:03:19.580
Poisson models are very useful for
modeling contingency table data.

44
00:03:19.580 --> 00:03:21.620
They give a very elegant framework for
doing that.

45
00:03:23.170 --> 00:03:29.004
I give the Poisson mass function here and
so

46
00:03:29.004 --> 00:03:36.450
the rate of counts per unit time is
lambda, whereas t is the total time.

47
00:03:36.450 --> 00:03:42.040
If x is a plus on with this mean,
then its expected value is t times lambda.

48
00:03:42.040 --> 00:03:48.840
So the expected value is plus sign
Is that's is the t times lambda.

49
00:03:48.840 --> 00:03:53.750
So our natural estimate of the rate would
be the count over the total time okay?

50
00:03:53.750 --> 00:03:58.380
So x over t and it's nice to know in
this case that the expected value of

51
00:03:58.380 --> 00:04:02.470
x over t the expected value of our
rate estimate is exactly lambda.

52
00:04:02.470 --> 00:04:05.547
The rate that we would like the estimate.

53
00:04:05.547 --> 00:04:08.940
So, that's the useful property
associated with the Poisson.

54
00:04:08.940 --> 00:04:12.210
The variance is equal to the mean,
so the variance is e lambda.

55
00:04:12.210 --> 00:04:15.480
So that's the assumption of our
model that we can check and

56
00:04:15.480 --> 00:04:19.020
we have some potential
solutions of it's doesn't hold.

57
00:04:20.420 --> 00:04:23.220
And another interesting fact is
the Poisson tends to a normal

58
00:04:23.220 --> 00:04:24.600
as the mean gets large.

59
00:04:24.600 --> 00:04:27.070
So you can think of this in several ways.

60
00:04:27.070 --> 00:04:29.570
All that has to happen is for
t lambda to get large.

61
00:04:29.570 --> 00:04:32.360
This could occur if t is fixed and

62
00:04:32.360 --> 00:04:37.260
lambda gets large, if lambda is fixed and
t gets large, or both of them get large.

63
00:04:37.260 --> 00:04:40.750
And in a lot of different
applications the way in which

64
00:04:40.750 --> 00:04:45.780
the mean gets large could vary but
as long as it gets large in some sense

65
00:04:45.780 --> 00:04:48.700
then the Poisson is going to
approximate a normal distribution.

66
00:04:48.700 --> 00:04:50.870
And here I show you this via simulation.

67
00:04:50.870 --> 00:04:55.330
I simulate three different collections
of Poisson random variables as

68
00:04:56.330 --> 00:04:59.060
the mean of the Poisson distribution
gets larger and larger and

69
00:04:59.060 --> 00:05:04.330
you can see by the right most
panel that it's nearly identical

70
00:05:04.330 --> 00:05:10.070
to a normal distribution at that point.

71
00:05:10.070 --> 00:05:14.180
And then,
we can actually show that we don't, if

72
00:05:14.180 --> 00:05:17.570
this isn't the appropriate class the fact
to show the mathematics that the meaning

73
00:05:17.570 --> 00:05:22.620
of variants are equal theoretically so,
a way could do that by simulation and

74
00:05:22.620 --> 00:05:28.390
I do that here where I right, are not,
I'm sorry this is an access simulation.

75
00:05:28.390 --> 00:05:32.340
We're actually try to show
it using the density and

76
00:05:32.340 --> 00:05:34.400
summing up the density in the right way.

77
00:05:34.400 --> 00:05:37.610
So if you're interested try that
experiment and it will prove to you

78
00:05:37.610 --> 00:05:41.210
that the meaning of variance or equal,
try it for bunch of different scenarios.

79
00:05:41.210 --> 00:05:44.460
Or you could just believe me or
you could take for example mathematical

80
00:05:44.460 --> 00:05:47.960
biostatistics boot camp one or
two are my other course or classes.

81
00:05:47.960 --> 00:05:50.590
Where we cover how to do
the actual mathematics for this.

82
00:05:52.840 --> 00:05:59.480
So as an example,
let's look at Jeff Leek, his web traffic.

83
00:05:59.480 --> 00:06:04.330
So this is his website, www.biostat, or

84
00:06:04.330 --> 00:06:09.750
I'm sorry, biostat.jhsph.edu/~jleek.

85
00:06:09.750 --> 00:06:17.500
And the place I mean in this case is the
interpretive a number of web hits per day.

86
00:06:17.500 --> 00:06:22.040
So our unit our time in this
case is T equal to one.

87
00:06:22.040 --> 00:06:27.030
Now for the one to interpret the length
that we estimate as web hits per hour we

88
00:06:27.030 --> 00:06:28.630
would have to put the T equal 24.

89
00:06:28.630 --> 00:06:30.800
So I hope you understand that and

90
00:06:30.800 --> 00:06:36.030
if you want it to have it to be seconds
you need to put a T equal 24 times or

91
00:06:36.030 --> 00:06:38.530
minutes it would have to be T
equal 24 times 60 and so on.

92
00:06:38.530 --> 00:06:42.950
Let's look at the data I show

93
00:06:42.950 --> 00:06:48.370
here how you can download it and
I convert the date from

94
00:06:48.370 --> 00:06:52.890
a standard character date
time format to a Julian date.

95
00:06:52.890 --> 00:06:58.280
Julian date counts the number of days
since January 1st 1970 I believe.

96
00:06:59.440 --> 00:07:02.470
So the Julian date is nice to think
about because it's just a count.

97
00:07:02.470 --> 00:07:05.620
It's the number of days whereas
the date is kind of a complicated

98
00:07:05.620 --> 00:07:06.770
format because it's characters.

99
00:07:06.770 --> 00:07:08.820
So when you do the head of the date here,

100
00:07:08.820 --> 00:07:11.250
you see the date which
is in character format.

101
00:07:11.250 --> 00:07:14.850
You see the number of visits,
and he is not doing so well.

102
00:07:14.850 --> 00:07:17.400
These early dates with 0
visits on all those dates.

103
00:07:17.400 --> 00:07:22.980
The number of visits that originate from
simply statistics and the julian date.

104
00:07:22.980 --> 00:07:25.020
So here's a plot of the data set.

105
00:07:25.020 --> 00:07:32.310
The Julian date is on the x axis and
the number of visits is on the y axis.

106
00:07:32.310 --> 00:07:37.030
Now, we've covered in the last
lecture what linear regression,

107
00:07:37.030 --> 00:07:41.830
some of the shortfalls of linear
regression is try to model count data or

108
00:07:41.830 --> 00:07:43.410
in that case, binary data.

109
00:07:43.410 --> 00:07:47.070
So let's not just re-hatch that same
topic, there are some issues with

110
00:07:47.070 --> 00:07:54.500
modelling count data as if it was
with a linear model directly.

111
00:07:54.500 --> 00:07:58.590
However, as we saw a couple of slides ago,
as the mean of the counts gets larger and

112
00:07:58.590 --> 00:08:02.730
larger are concerned over
this decreases quite a bit

113
00:08:02.730 --> 00:08:05.380
simply because it's going to
trend to a normal distribution.

114
00:08:05.380 --> 00:08:09.400
So, if you have extremely large counts,
this becomes a lot less objectionable.

115
00:08:10.530 --> 00:08:16.430
So, that's just for notation number of
heads, NH is going to be our outcome JD,

116
00:08:16.430 --> 00:08:21.030
is the Julian day, that's going to be
our predictor and this would be a linear

117
00:08:21.030 --> 00:08:27.080
regression model, we can plot it and
see the fitted line that we would get.

118
00:08:27.080 --> 00:08:28.010
It has some issues.

119
00:08:28.010 --> 00:08:29.710
Clearly there's some curvature there,

120
00:08:29.710 --> 00:08:32.390
maybe we should have put
an x squared term in.

121
00:08:32.390 --> 00:08:36.010
But that would be our first
approach to this, and

122
00:08:36.010 --> 00:08:37.320
honestly it wouldn't be that bad.

123
00:08:37.320 --> 00:08:41.540
But the counts are kind of small, so
it's not the best thing in the world.

124
00:08:41.540 --> 00:08:44.430
The interpretation isn't great for
linear models,

125
00:08:44.430 --> 00:08:47.260
then we'll see some ways which
in the next couple of slides,

126
00:08:47.260 --> 00:08:50.830
how we can tweak linear models to maybe
get a slightly better interpretation.

127
00:08:50.830 --> 00:08:52.380
I think that of counts in web hits and

128
00:08:52.380 --> 00:08:56.190
things like that as things that you would
want to think about on a relative scale

129
00:08:56.190 --> 00:08:59.700
and the linear model really treats
it on a linear additive scale.

130
00:08:59.700 --> 00:09:02.450
So let's think about how we could get

131
00:09:02.450 --> 00:09:05.650
relative interpretations
from our linear model.

132
00:09:05.650 --> 00:09:08.797
The first thing we might try is
taking the log of the outcome,

133
00:09:08.797 --> 00:09:10.289
here I knew the natural log.

134
00:09:11.810 --> 00:09:17.001
And this would be our model,
log of NH is the linear regression model.

135
00:09:17.001 --> 00:09:20.351
B0 + B1(JDi) + ei.

136
00:09:21.380 --> 00:09:25.520
Now let me speak a little bit about
log and what it's accomplishing.

137
00:09:25.520 --> 00:09:28.460
The quantity e to the expected
value of the log of a random

138
00:09:28.460 --> 00:09:31.900
variable is what I would call
the population geometric mean.

139
00:09:31.900 --> 00:09:36.060
And the reason I would call it the
population geometric mean is the empirical

140
00:09:36.060 --> 00:09:40.560
or just geometric mean is
the product of a sample,

141
00:09:40.560 --> 00:09:43.109
product Yi,
raised to the one over n power.

142
00:09:44.320 --> 00:09:50.350
So this the way to think about this,
the product of yi to the one over n power.

143
00:09:50.350 --> 00:09:53.450
If we take a log of that,
we get the arithmatic mean,

144
00:09:53.450 --> 00:09:56.260
the ordinary mean of the log data.

145
00:09:56.260 --> 00:09:59.290
So the geometric mean
is just exponentiating

146
00:09:59.290 --> 00:10:01.220
the arithmatic mean of the log data.

147
00:10:02.440 --> 00:10:07.610
And we know that if we collect a lot
of data, a lot more data in our sample,

148
00:10:07.610 --> 00:10:10.620
the arithmetic mean will
converge to something.

149
00:10:10.620 --> 00:10:13.950
So the geometric mean
is what this quantity,

150
00:10:13.950 --> 00:10:18.750
the product of the data, rays to the one
over nth power, what it converges to.

151
00:10:18.750 --> 00:10:23.800
So, what, it turns out,
when you take the log of the natural log

152
00:10:23.800 --> 00:10:28.090
of the outcome in a linear
regression then, your exponentiated

153
00:10:28.090 --> 00:10:33.740
coefficients are interpretable
with respect to geometric means.

154
00:10:33.740 --> 00:10:38.020
So, for example, E to the Beta of zero is
the estimated geometric mean hits on day

155
00:10:38.020 --> 00:10:41.680
zero and I should reiterate the point
from earlier on in the class.

156
00:10:41.680 --> 00:10:45.690
This intercept doesn't mean that much
because January first 1970 is not

157
00:10:45.690 --> 00:10:50.970
a date that we care about in
terms of number of web hits.

158
00:10:50.970 --> 00:10:56.240
So probably to make the intercept more
interpretable, what we should have done is

159
00:10:56.240 --> 00:11:02.910
subtracted off the earliest date that we
saw and started counting days from there.

160
00:11:02.910 --> 00:11:05.670
From all of the remaining
days in our data set and

161
00:11:05.670 --> 00:11:10.440
then the intercept would be the e to
the inner estimated intercept would be

162
00:11:10.440 --> 00:11:15.040
the geometric mean hits on
the first day of this data set.

163
00:11:15.040 --> 00:11:17.780
Okay.
So that's a small point but

164
00:11:17.780 --> 00:11:18.990
it doesn't change the fitted model.

165
00:11:18.990 --> 00:11:22.330
It doesn't change the slope or anything
like that to shift around the intercept

166
00:11:22.330 --> 00:11:26.290
however nonetheless, if you want
an interpretable intercept as we know

167
00:11:26.290 --> 00:11:31.220
from earlier on in the class,
you have to do something like that.

168
00:11:31.220 --> 00:11:35.760
E to the beta1 on the other hand is
the estimated relative increase or

169
00:11:35.760 --> 00:11:39.570
decrease in the geometric
mean hits per day, okay?

170
00:11:40.820 --> 00:11:42.570
So the increase per day.

171
00:11:44.650 --> 00:11:47.530
So I should also mention
there's a problem with logs.

172
00:11:47.530 --> 00:11:51.500
If you have zero counts you have to do
something because you can't take the log

173
00:11:51.500 --> 00:11:53.480
of zero, so you need to add a constant.

174
00:11:53.480 --> 00:11:55.600
A very common constant that is plus one.

175
00:11:55.600 --> 00:12:00.690
So we do log of the outcome plus one.

176
00:12:00.690 --> 00:12:04.840
So if we do that, here I fit the linear
model to the log of the outcome plus one

177
00:12:04.840 --> 00:12:05.870
versus the Julian date.

178
00:12:05.870 --> 00:12:09.060
We get the intercept which is kind of
irrelevant in this case as we talked

179
00:12:09.060 --> 00:12:09.970
about before.

180
00:12:09.970 --> 00:12:11.470
And then we get 1.002.

181
00:12:11.470 --> 00:12:14.050
This is on the exponentiated scale.

182
00:12:14.050 --> 00:12:14.551
Okay so

183
00:12:14.551 --> 00:12:20.568
what that means is our model is estimating
a 0.2% increase in web traffic per day.

184
00:12:20.568 --> 00:12:21.600
Okay?

185
00:12:21.600 --> 00:12:23.080
And that's a nice interpretation.

186
00:12:23.080 --> 00:12:26.890
If you added other
covariates then that would

187
00:12:26.890 --> 00:12:29.530
be 0.02% increase per day holding
the other covariates fixed.