WEBVTT

1
00:00:00.210 --> 00:00:04.690
Hi, my name is Brian Caffo and this is
a lecture on regression to the mean,

2
00:00:04.690 --> 00:00:09.070
as part of the Coursera regression class
in the data science specialization.

3
00:00:09.070 --> 00:00:12.540
This class is co-taught with my
co-instructors, Jeff Leek and Roger Peng.

4
00:00:12.540 --> 00:00:14.530
We're all in the department
of biostatistics

5
00:00:14.530 --> 00:00:17.680
at the Johns Hopkins Bloomberg School
of Public Health.

6
00:00:17.680 --> 00:00:21.720
Regression to the mean was an important
milestone in the discovery of regression.

7
00:00:21.720 --> 00:00:22.690
So we're going to talk about it.

8
00:00:22.690 --> 00:00:24.630
It was discovered by Francis Galton.

9
00:00:26.940 --> 00:00:29.956
Regression to mean ask
questions like this.

10
00:00:29.956 --> 00:00:35.160
Why is that the children of very tall
parents also tend to be tall, but

11
00:00:35.160 --> 00:00:37.380
not quite as tall as their parents?

12
00:00:37.380 --> 00:00:41.750
And also, why are the children with very
short parents tend also to be very short,

13
00:00:41.750 --> 00:00:43.200
but not quite as short as their parents.

14
00:00:44.410 --> 00:00:47.530
We can also reverse whether we're talking
about the parents and the children.

15
00:00:47.530 --> 00:00:51.880
Why do the parents of very short
children tend to also be short,

16
00:00:51.880 --> 00:00:53.710
but not quite as short as their children?

17
00:00:53.710 --> 00:00:58.830
And why do the parents of very tall
children tend also to, to be tall,

18
00:00:58.830 --> 00:01:00.880
but not quite as tall as their children?

19
00:01:03.260 --> 00:01:06.850
This often come, often comes up
when talking about athletes.

20
00:01:06.850 --> 00:01:10.270
Athletes who are the best performing
one year tend to do a little bit worse

21
00:01:10.270 --> 00:01:13.650
the next year and athletes who
are the worst performing one year

22
00:01:13.650 --> 00:01:15.820
tend to do a little bit
better the next year.

23
00:01:15.820 --> 00:01:18.107
Often people talk about
stocks in the same way.

24
00:01:18.107 --> 00:01:21.337
Some of the best performing
stocks tend to go down.

25
00:01:21.337 --> 00:01:24.599
We'll talk about why this is and
whether or not something is intrinsic or

26
00:01:24.599 --> 00:01:28.300
whether it's a regression to the mean
effect, which is the important question.

27
00:01:30.260 --> 00:01:33.340
These phenomena are all examples of so
called regression to the mean.

28
00:01:34.390 --> 00:01:37.190
Regression to the mean,
as I mentioned in the previous slide,

29
00:01:37.190 --> 00:01:39.700
was invented by Francis Galton
in this famous paper,

30
00:01:39.700 --> 00:01:42.559
regression towards mediocrity in hered,
hereditary stature.

31
00:01:44.310 --> 00:01:46.360
I like to think of regression to the mean

32
00:01:47.570 --> 00:01:50.470
by thinking of the case where it's
a 100% regression to the mean.

33
00:01:51.600 --> 00:01:55.200
So imagine if I were to simulate
pairs of standard normals.

34
00:01:55.200 --> 00:01:56.700
So they have nothing to
do with one another,

35
00:01:56.700 --> 00:01:58.020
they're independent standard normals.

36
00:02:00.030 --> 00:02:04.650
Well, in this first vector of
standard normals that I generate.

37
00:02:04.650 --> 00:02:06.670
If I were to take the largest one,

38
00:02:06.670 --> 00:02:10.380
the chance that its pair in the second
vector is smaller will be high.

39
00:02:10.380 --> 00:02:16.180
And this is simply saying that
the probability that Y is less than X,

40
00:02:16.180 --> 00:02:20.730
given X is going to get bigger
as X heads to very large values.

41
00:02:22.250 --> 00:02:25.960
The same thing in other words,
is that probability Y is greater than X.

42
00:02:25.960 --> 00:02:29.260
Given that X equals X is going to get
bigger as X heads to smaller values.

43
00:02:32.430 --> 00:02:35.160
This extreme version of
regression in the mean where

44
00:02:35.160 --> 00:02:38.950
there's 100% regression to the mean
is what I like to think about.

45
00:02:38.950 --> 00:02:44.235
However, in most cases
there's some blend of some,

46
00:02:44.235 --> 00:02:48.200
some intrinsic component, and a noise.

47
00:02:48.200 --> 00:02:53.330
Take, for example, if I were to give
every student in this class two quizzes.

48
00:02:53.330 --> 00:02:54.530
Two very hard quizzes.

49
00:02:55.940 --> 00:02:59.120
While the top performers likely
do know the material very better,

50
00:02:59.120 --> 00:03:01.672
do know the, the, the material better.

51
00:03:03.440 --> 00:03:07.430
However, quizzes
are imperfect instruments.

52
00:03:07.430 --> 00:03:09.910
And heaven knows if I gave the quiz,

53
00:03:09.910 --> 00:03:13.830
there's going to be a lot of error in
that, in the quiz as an instrument.

54
00:03:13.830 --> 00:03:15.650
So there's some noise.

55
00:03:15.650 --> 00:03:18.390
So the top performer might
either is probably a little

56
00:03:18.390 --> 00:03:20.840
knows the material a little
bit better than everyone else.

57
00:03:20.840 --> 00:03:23.240
But also may have
benefitted from some noise.

58
00:03:23.240 --> 00:03:25.960
So in the second quiz,
they might perform a little bit worse.

59
00:03:25.960 --> 00:03:28.520
Probably still well,
but a little bit worse.

60
00:03:28.520 --> 00:03:30.250
The same thing for the worst performers.

61
00:03:31.880 --> 00:03:36.950
It's often kind of fun to think about,
how much of the discussion

62
00:03:36.950 --> 00:03:40.440
of sports is really just talking
about regression to the mean.

63
00:03:40.440 --> 00:03:44.680
A person who has a phenomenal year in
baseball in their batting average one year

64
00:03:44.680 --> 00:03:47.030
has a slightly lower one the next year.

65
00:03:47.030 --> 00:03:48.470
Is that just regression to the mean?

66
00:03:48.470 --> 00:03:51.028
It would be nice to figure
out how to quantify it.

67
00:03:51.028 --> 00:03:55.041
This is what Francis Galton did with
regression in the first treatment of

68
00:03:55.041 --> 00:03:56.436
regression in the mean.

69
00:03:59.316 --> 00:04:03.421
We want to talk about how Francis Galton
used the idea of regression, and

70
00:04:03.421 --> 00:04:09.320
particularly correlation, which we knows
intimately related to linear regression.

71
00:04:09.320 --> 00:04:13.340
So particularly correlation,
to quantify regression to the mean, and

72
00:04:13.340 --> 00:04:15.570
we're going to basically
do it with a picture, but

73
00:04:15.570 --> 00:04:18.240
I want to set up the picture first
before I go through the r code.

74
00:04:21.270 --> 00:04:23.877
In this case,
my X is going to be the child's height and

75
00:04:23.877 --> 00:04:26.360
Y is going to be the parent's height.

76
00:04:26.360 --> 00:04:28.882
I'm going to use a dataset where
the parent is a single parent,

77
00:04:28.882 --> 00:04:29.930
the father in this case.

78
00:04:31.710 --> 00:04:35.830
These data have all been normalized,
the Y and the X, so

79
00:04:35.830 --> 00:04:37.840
they have 0 mean and unit variance.

80
00:04:37.840 --> 00:04:40.890
And we should be very familiar with
this by now, being able to do that.

81
00:04:42.220 --> 00:04:45.600
So recall if I've done that then
my regression line is going to

82
00:04:45.600 --> 00:04:47.096
pass through the point 0, 0.

83
00:04:48.740 --> 00:04:53.230
And also the slope, regardless of whether
the child's height is the outcome or

84
00:04:53.230 --> 00:04:56.100
the parent's height is the outcome,
is now simply the correlation.

85
00:04:57.700 --> 00:05:03.620
I want to also mention just a quick kind
of quirk of, of creating the plot is if

86
00:05:03.620 --> 00:05:09.750
X is the outcome and you happen to plot
the outcome on the horizontal axis,

87
00:05:09.750 --> 00:05:14.390
the slope of the line that you plot, needs
to have the slope 1 over the correlation.

88
00:05:14.390 --> 00:05:18.094
And that's just simply because
I plotted it on the X,

89
00:05:18.094 --> 00:05:20.577
I plotted the outcome on the X axis.

90
00:05:20.577 --> 00:05:24.550
So just to remind you of that if you
get a little confused with a code.

91
00:05:24.550 --> 00:05:26.278
So let's go through the code and

92
00:05:26.278 --> 00:05:30.374
then once we get the plot that I'm going
to show you, we'll go through the,

93
00:05:30.374 --> 00:05:34.406
the different parts of the plot to talk
about how looking at the correlation

94
00:05:34.406 --> 00:05:38.025
quantified regression of the me,
regression of the mediocrity.

95
00:05:38.025 --> 00:05:44.395
[SOUND] So let's go through the code and
how I'm creating my plot.

96
00:05:44.395 --> 00:05:49.270
So the dataset is in the usingR library,
the data's father.son.

97
00:05:49.270 --> 00:05:52.540
I'm going to define my y
as the son's heights, but

98
00:05:52.540 --> 00:05:55.370
I'm going to subtract of the mean and
divide by the standard deviation.

99
00:05:55.370 --> 00:05:59.330
I'm going to defi,
define my x as the father's heights and

100
00:05:59.330 --> 00:06:00.940
I'm going to do the same.

101
00:06:00.940 --> 00:06:03.890
Now x and y should both have mean 0 and
variance 1, and

102
00:06:03.890 --> 00:06:04.860
you can check that yourself.

103
00:06:06.080 --> 00:06:09.750
Rho is the standard Greek letter that
we use to represent correlations.

104
00:06:09.750 --> 00:06:15.600
So I'm going to define rho as my
correlation between my x and my y.

105
00:06:15.600 --> 00:06:18.120
I need to load ggplot 2.

106
00:06:18.120 --> 00:06:23.160
And before I show you the plot,
let's check a couple of things.

107
00:06:23.160 --> 00:06:24.559
Let's see what rho works out to be.

108
00:06:24.559 --> 00:06:27.156
It works out to be about 0.5.

109
00:06:28.280 --> 00:06:31.630
So the correlation between the father's
height and the son's height was 0.5.

110
00:06:31.630 --> 00:06:36.860
So now let's go back up here and
I'll create my plot.

111
00:06:36.860 --> 00:06:40.430
I'm going to do create assign
to the variable g, my ggplot.

112
00:06:40.430 --> 00:06:45.248
I'm going to add the points.

113
00:06:45.248 --> 00:06:48.050
But I'm going to do it in a way that
I have black in the background and

114
00:06:48.050 --> 00:06:49.650
a salmon color on the foreground.

115
00:06:49.650 --> 00:06:53.230
And that alpha blending makes
the points kind of transparent.

116
00:06:53.230 --> 00:06:57.210
I want my x limits to be minus
4 to plus 4 on both axis.

117
00:06:57.210 --> 00:06:58.720
I want my axis to be the same.

118
00:06:58.720 --> 00:07:04.168
I also know that minus 4 to plus 4
should be pretty much all encompassing,

119
00:07:04.168 --> 00:07:07.120
all encompassing of the data.

120
00:07:07.120 --> 00:07:11.480
The probability of that, for example,
standard-normal, is below minus 4 or

121
00:07:11.480 --> 00:07:13.662
above plus 4 is extremely low.

122
00:07:13.662 --> 00:07:15.360
And Chebyshev's theorem also helps, so

123
00:07:15.360 --> 00:07:18.280
if you've had the inference class,
you'll know that

124
00:07:18.280 --> 00:07:22.900
a standardized random variable minus 4 to
plus 4 is pretty far out into the tail.

125
00:07:25.200 --> 00:07:28.610
I'm going to, and, and let me show you
this plot before I add anything more,

126
00:07:28.610 --> 00:07:30.000
any more layers to it.

127
00:07:30.000 --> 00:07:30.700
So there's my plot.

128
00:07:30.700 --> 00:07:35.660
Now, I'm going to add a layer
that is the identity line.

129
00:07:35.660 --> 00:07:36.880
Okay.
Let's show that.

130
00:07:38.480 --> 00:07:45.769
Then I'm going to add the axes,
the horizontal and vertical axes.

131
00:07:45.769 --> 00:07:46.878
Let's show that.

132
00:07:49.458 --> 00:07:56.134
Oh, and there's a slight mistake
that this should be h line.

133
00:07:56.134 --> 00:08:01.843
[SOUND] Now, let's show that so
that now I actually have my two axis.

134
00:08:01.843 --> 00:08:06.574
Okay, and now I'm going to create
the line where I treat the son's

135
00:08:06.574 --> 00:08:11.710
height as the outcome and
the father's height as the predictor.

136
00:08:11.710 --> 00:08:16.000
I'm going to add the line now
where I treat the son's height

137
00:08:16.000 --> 00:08:19.440
as the predictor and
the father's height as the outcome.

138
00:08:19.440 --> 00:08:24.390
Of course, the axes are rotated as I
discussed on the previous slide, and

139
00:08:24.390 --> 00:08:26.510
so I need the slope to be 1 over rho.

140
00:08:27.640 --> 00:08:33.870
Now let me get the plot I've created for
the two fitted lines.

141
00:08:33.870 --> 00:08:37.290
I've doubled the width
of the fitted lines.

142
00:08:38.860 --> 00:08:43.537
So next let's just talk about regression
to the mean as it relates to this plot.

143
00:08:51.129 --> 00:08:55.550
Let's look at this plot, and describe,
use it to describe regression of the mean.

144
00:08:55.550 --> 00:09:00.150
If the observations fell perfectly on
a line it would have to be, in this case,

145
00:09:00.150 --> 00:09:03.590
the identity line because we've
normalized both the X and the Y.

146
00:09:03.590 --> 00:09:06.820
So they would fall exactly
on this identity line here.

147
00:09:06.820 --> 00:09:10.374
Just as a reminder,
the father's height is plotted,

148
00:09:10.374 --> 00:09:15.332
plotted as the X variable and the son's
height is plotted as the Y variable.

149
00:09:15.332 --> 00:09:20.694
So if we have a father's height of 2 and
there were no noise,

150
00:09:20.694 --> 00:09:25.330
we would predict the son's
height to also be 2.

151
00:09:25.330 --> 00:09:28.853
Here, the 2 representing 2 standard
deviations above the mean for

152
00:09:28.853 --> 00:09:32.330
the fathers and 2 standard deviations
above the mean for the sons.

153
00:09:34.280 --> 00:09:36.450
However, there is lots of noise
when we look at the data,

154
00:09:36.450 --> 00:09:38.440
there's quite a bit of noise.

155
00:09:38.440 --> 00:09:43.799
So the prediction is now, not 2, but
it's on the regression line, right here.

156
00:09:45.790 --> 00:09:49.765
And because of how we set things
up this is simply multiplying

157
00:09:49.765 --> 00:09:54.470
2 times the slope which
is the correlation.

158
00:09:54.470 --> 00:10:00.980
And now the prediction comes over here,
which is between 2 and 0.

159
00:10:02.380 --> 00:10:05.830
And in fact that is exactly 2
multiplied times the correlation.

160
00:10:05.830 --> 00:10:08.150
That is the regression of the mean.

161
00:10:08.150 --> 00:10:13.290
That multiplication is how Francis Galton
measured regression of the mean.

162
00:10:13.290 --> 00:10:18.480
How shrunken this correlation is

163
00:10:18.480 --> 00:10:23.750
towards horizontal line gives you
the extent of the regression to the mean.

164
00:10:26.440 --> 00:10:31.180
If there, we, we talked about the case
where there was no noise, and this line

165
00:10:31.180 --> 00:10:36.510
would fall perfectly on the identity line
consider the case where it's all noise.

166
00:10:36.510 --> 00:10:40.120
The father's height had no
information about the son's height.

167
00:10:40.120 --> 00:10:42.550
Then this line,
the correlation would be 0.

168
00:10:42.550 --> 00:10:46.527
This line would be right
on the horizontal axis.

169
00:10:46.527 --> 00:10:51.929
Then if you had to predict the father's,
the son's height from the father's height,

170
00:10:51.929 --> 00:10:54.931
you would always wind up
with a prediction of 0.

171
00:10:54.931 --> 00:10:59.281
So this phenomenon is regression
to the mean, and of course,

172
00:10:59.281 --> 00:11:03.882
we can do the same thing with it flipped,
if we consider the son's

173
00:11:03.882 --> 00:11:08.900
height as the predictor, and
the father's height as the outcome.

174
00:11:08.900 --> 00:11:13.380
Then you can visualize this on this
plot because I've shown, basically,

175
00:11:13.380 --> 00:11:18.650
that plot with this line right here, and

176
00:11:18.650 --> 00:11:25.130
the regression to the mean is how
shrunken it is toward the vertical axis.

177
00:11:25.130 --> 00:11:28.873
It's a pretty clever idea and
I, I think this, this was one

178
00:11:28.873 --> 00:11:34.310
of the fundamental ideas that led to what
we now think of as modern regression.

179
00:11:34.310 --> 00:11:37.280
However, the idea of
regression to the mean still

180
00:11:37.280 --> 00:11:39.390
has an important place in statistics.

181
00:11:39.390 --> 00:11:42.420
For example,
when you study longitudinal data,

182
00:11:42.420 --> 00:11:45.630
it's important to think about
the idea of regression to the mean.