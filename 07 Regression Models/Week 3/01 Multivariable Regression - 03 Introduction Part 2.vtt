WEBVTT

1
00:00:00.670 --> 00:00:03.860
So let's talk about how you
actually get the estimates,

2
00:00:03.860 --> 00:00:06.940
or how R or
whatever figures these things out.

3
00:00:06.940 --> 00:00:10.070
And I'm gonna go through a derivation,
and again, if this is not your thing,

4
00:00:10.070 --> 00:00:12.510
then feel free to skip over
this part of the lecture.

5
00:00:14.540 --> 00:00:18.000
Remember if you have
regression to the origin,

6
00:00:18.000 --> 00:00:20.640
you want a line that's forced to
the origin that has no intercepts.

7
00:00:20.640 --> 00:00:24.270
You have the single predictor x and
a single predictor of y and

8
00:00:24.270 --> 00:00:26.260
you want no intercept.

9
00:00:26.260 --> 00:00:30.890
The slope estimate was just this
sum of the product x times y

10
00:00:30.890 --> 00:00:32.670
divided by the sum of the x is squared.

11
00:00:32.670 --> 00:00:35.170
That, we talked about that
earlier on in the class.

12
00:00:36.300 --> 00:00:39.070
Now let's just consider,
let's see if we can use that result to

13
00:00:39.070 --> 00:00:42.590
derive the least squares estimate
when we have two regressors.

14
00:00:42.590 --> 00:00:45.620
And then I think if you've got the gist
of this you can see how it would work for

15
00:00:45.620 --> 00:00:48.720
three and how it would work for
four and so on.

16
00:00:48.720 --> 00:00:52.160
Okay, so derive is maybe a bit of
an ambitious term as to what I

17
00:00:52.160 --> 00:00:53.420
intend to do here.

18
00:00:53.420 --> 00:00:55.850
What I guess I mean is develop,
and I'm gonna ask for

19
00:00:55.850 --> 00:00:59.230
those that are interested to fill
in the details on their own.

20
00:00:59.230 --> 00:01:02.230
But what I'm gonna do is simply
develop the estimator a little bit.

21
00:01:02.230 --> 00:01:05.400
And the reason I'm gonna go through
this development is I think it

22
00:01:05.400 --> 00:01:08.960
illustrates what multivariable
regression is doing.

23
00:01:08.960 --> 00:01:12.240
If you're not too interested in this sort
of development, then I think you could

24
00:01:12.240 --> 00:01:17.880
reasonably skip over this slide or
this set of notes and not worry about it.

25
00:01:17.880 --> 00:01:20.430
But if you're interested, and
you kinda wanna understand

26
00:01:20.430 --> 00:01:25.870
how multi-variable regression works,
I'm gonna give a development that's

27
00:01:25.870 --> 00:01:30.010
more intuitive than what you would get
with something like linear algebra.

28
00:01:31.470 --> 00:01:34.940
In addition I have some videos on YouTube
where I go through the linear algebra

29
00:01:34.940 --> 00:01:36.990
development if you'd like to watch those.

30
00:01:36.990 --> 00:01:39.760
But I don't think that that's actually
necessary to sort of understand

31
00:01:39.760 --> 00:01:41.100
what's going on.

32
00:01:41.100 --> 00:01:43.270
So let's consider two regressors.

33
00:01:43.270 --> 00:01:48.270
We have summation of the outcome yi,

34
00:01:48.270 --> 00:01:52.480
minus the first regressor
Xi1 times Beta 1,

35
00:01:52.480 --> 00:01:57.910
minus the second regressor,
Xi2, X2i times Beta 2 squared.

36
00:01:57.910 --> 00:02:02.410
That's the least squares criteria
that we would like to minimize.

37
00:02:02.410 --> 00:02:08.160
So for example,
X1i might just be an intercept and

38
00:02:08.160 --> 00:02:14.890
X2i might be a covariant of interest,
blood pressure or something like that.

39
00:02:14.890 --> 00:02:16.840
Okay, so here's the trick.

40
00:02:18.500 --> 00:02:22.650
Imagine if I were to just know beta 1 or
fix beta 1.

41
00:02:22.650 --> 00:02:28.166
Then I get yi~ let's say is equal

42
00:02:28.166 --> 00:02:33.690
to yi minus- x1i times beta 1.

43
00:02:33.690 --> 00:02:39.645
And this equation becomes summation

44
00:02:39.645 --> 00:02:45.030
yi~ minus X2i beta 2 squared.

45
00:02:46.840 --> 00:02:52.150
Well, then this is exactly

46
00:02:52.150 --> 00:02:56.945
regression through the origin
with just the single regressor,

47
00:02:56.945 --> 00:03:01.840
X2y but the outcome y~i.

48
00:03:01.840 --> 00:03:05.960
So we know what the estimate for
beta 2 would have to be.

49
00:03:05.960 --> 00:03:08.785
It would have to be summation yi~.

50
00:03:08.785 --> 00:03:15.890
x2i over summation x2i squared.

51
00:03:15.890 --> 00:03:19.710
Now, what you can do, this,
and remember, yi~, see,

52
00:03:19.710 --> 00:03:21.290
remember its definition over here.

53
00:03:21.290 --> 00:03:25.010
If we were to plug it in,
that's a function of beta 1.

54
00:03:25.010 --> 00:03:30.690
So what I could do is now that I know that
my beta 2 has to satisfy this equation.

55
00:03:30.690 --> 00:03:37.340
I can take this and
plug it back in here, right.

56
00:03:37.340 --> 00:03:39.290
Or equivalently back in here.

57
00:03:40.880 --> 00:03:45.270
And then what that yields
is an equation for

58
00:03:45.270 --> 00:03:48.680
that only involves beta 1 and
a regression through the origin for

59
00:03:48.680 --> 00:03:51.639
beta 1 and if you want to you can
fill in the rest of the details.

60
00:03:52.730 --> 00:03:56.530
What it works out to be, and
this is the interesting part,

61
00:03:56.530 --> 00:04:00.940
is that the regression slope for beta one,

62
00:04:02.310 --> 00:04:09.210
is exactly what you would obtain if
you took the residual of x2 out of x1,

63
00:04:09.210 --> 00:04:14.210
and x2 out of y and
then just did regression to the origin.

64
00:04:14.210 --> 00:04:19.500
So kind of what multi-variable regression
is doing is it's the coefficient for

65
00:04:19.500 --> 00:04:24.160
x1, say beta 1,
is the coefficient having removed x2,

66
00:04:24.160 --> 00:04:29.680
the other covariant, from both
the response and that first predictor, x1.

67
00:04:29.680 --> 00:04:33.180
Similarly, the regression coefficient for

68
00:04:33.180 --> 00:04:37.845
x2 beta 2 is going to be
what you get if you were to

69
00:04:37.845 --> 00:04:42.860
remove x1 out of both the response y and
out of the second predictor, x2.

70
00:04:42.860 --> 00:04:45.110
We're gonna go over this point a lot and

71
00:04:45.110 --> 00:04:47.730
I know it's probably
confusing to begin with.

72
00:04:47.730 --> 00:04:51.550
But it explains what multi-variable
regression is doing.

73
00:04:51.550 --> 00:04:54.230
A coefficient from
a multi-variable regression

74
00:04:54.230 --> 00:04:58.100
is the coefficient where all
the other variables have been.

75
00:04:58.100 --> 00:05:02.120
Their linear effect on that predictor and
their response has been removed.

76
00:05:02.120 --> 00:05:04.570
And this is a very useful fact.

77
00:05:04.570 --> 00:05:06.660
And we'll exploit it in a variety of ways.

78
00:05:08.700 --> 00:05:12.760
So in case you skipped over that previous
slide I wanted to go through the result.

79
00:05:12.760 --> 00:05:13.900
Because it's important for

80
00:05:13.900 --> 00:05:15.960
your understanding of
multi-variable regression.

81
00:05:17.220 --> 00:05:21.740
So remember, we have two covariants
in this case, an X1 and an X2.

82
00:05:21.740 --> 00:05:23.870
And of course, one outcome Y.

83
00:05:24.910 --> 00:05:29.510
And we're interested in a model that
has only the two covariants and

84
00:05:29.510 --> 00:05:33.410
a regression slope Beta 1 and then
regression slope Beta 2 for the second.

85
00:05:35.250 --> 00:05:40.250
The result is that the fitted coefficient
for beta 1 is what you would get

86
00:05:40.250 --> 00:05:45.410
with regression through the origin if I
had removed the second coefficient X2.

87
00:05:45.410 --> 00:05:51.010
If I had removed its linear effect
from both the response Y and

88
00:05:51.010 --> 00:05:52.780
the first regression variable X1.

89
00:05:55.260 --> 00:05:59.730
So that's the sense in which multivariable
regression, the sense in which

90
00:05:59.730 --> 00:06:05.870
the coefficient for X1 has been adjusted
for the effect of X2 because the linear

91
00:06:05.870 --> 00:06:11.500
fact of X2 has been removed from both
the response Y and the first predictor X1.

92
00:06:12.650 --> 00:06:19.060
Similarly, the same thing could be said
about the coefficient for X2 beta 2.

93
00:06:19.060 --> 00:06:20.670
Once we get its estimate,

94
00:06:20.670 --> 00:06:25.430
beta 2 hat, it is what you would get
if you were to remove the effect,

95
00:06:25.430 --> 00:06:31.550
the linear effect of X1 out of both the
response Y, and the second predictor, X2.

96
00:06:33.090 --> 00:06:37.270
And this is why multivariable regression
relationships are sort of thought of as

97
00:06:37.270 --> 00:06:39.920
having been adjusted for
all the other variables.

98
00:06:39.920 --> 00:06:43.760
We'll go on in the next slide and
talk about how this relates

99
00:06:43.760 --> 00:06:48.140
to a full multivariable regression,
where you have lots of predictors.

100
00:06:48.140 --> 00:06:52.300
So let's go through the case that we
already know, of simple linear regression.

101
00:06:52.300 --> 00:06:54.970
Where one of our co-variances
is an intercept term and

102
00:06:54.970 --> 00:06:57.500
the other is some co-variant of interest.

103
00:06:57.500 --> 00:07:03.270
So here we have Yi equals Beta 1 X1i, plus
Beta 2, X2i, where in this case I'm just

104
00:07:03.270 --> 00:07:07.190
gonna assume that the second one is the
intercept term whereas we usually assume

105
00:07:07.190 --> 00:07:09.699
that the first one is the intercept
term but that of course doesn't matter.

106
00:07:12.010 --> 00:07:17.380
So if we adhere to my rule that the beta
one co, the estimated beta one coefficient

107
00:07:17.380 --> 00:07:22.510
is gonna be the one that's obtained by
getting rid of X2 out of the both Y term

108
00:07:22.510 --> 00:07:27.930
and the X1 term, let's think about what
we would get if we fitted X2 on Y.

109
00:07:27.930 --> 00:07:29.950
Well X2 is just an intercept.

110
00:07:29.950 --> 00:07:35.410
So as we know in intercept only regression
the fitted value is just Y bar.

111
00:07:35.410 --> 00:07:39.330
And so the residuals are just
the observations Yi minus Y bar.

112
00:07:41.150 --> 00:07:42.920
In other words the centered
version of the Ys.

113
00:07:42.920 --> 00:07:48.870
And then if we were to fit X2 on X1,
again, X2 is just a constant.

114
00:07:48.870 --> 00:07:51.676
If we were to do the fit,
the fitted values,

115
00:07:51.676 --> 00:07:55.820
the coefficient would just be X1 bar,
the mean of that covariant.

116
00:07:55.820 --> 00:07:59.720
And so the residuals would
just be X1i minus X bar,

117
00:07:59.720 --> 00:08:01.670
the centered version of that covariant.

118
00:08:01.670 --> 00:08:06.070
So, getting rid of this intercept term
from the Y is just centering that variable

119
00:08:06.070 --> 00:08:11.420
and getting rid of this intercept term
from X1 is just centering that covariant.

120
00:08:11.420 --> 00:08:14.840
And then my fitted regression
estimate is given down here,

121
00:08:14.840 --> 00:08:18.800
is just the product of the centered
Ys times the centered Xs,

122
00:08:18.800 --> 00:08:22.680
added up, divided by the sum
of the centered Xs squared.

123
00:08:22.680 --> 00:08:25.490
I give you that formula here and you can
work with it and it just works out to be

124
00:08:25.490 --> 00:08:28.720
the standard regression slope,
the correlation between the Xs and

125
00:08:28.720 --> 00:08:31.254
the Ys, times the ratio of
the standard deviations.

126
00:08:33.200 --> 00:08:36.730
So now let's extend this to
more than two variables, so

127
00:08:36.730 --> 00:08:40.570
that we can start dipping into
the idea of multivariable regression.

128
00:08:41.740 --> 00:08:45.670
So hopefully you see where we're
going with multivariable regression.

129
00:08:45.670 --> 00:08:50.230
Our least squares criteria is now the
distance between our observed outcome, Yi,

130
00:08:51.380 --> 00:08:56.960
and X1 times its coefficient all
the way up to Xp times its coefficient,

131
00:08:56.960 --> 00:08:58.900
the squared distances between those.

132
00:09:00.560 --> 00:09:03.670
If we only have two regressors,
an intercept and

133
00:09:03.670 --> 00:09:08.770
a slope, this amounts to minimizing
the sum of the squared vertical distances

134
00:09:08.770 --> 00:09:12.880
between the outcomes Ys and the predictors
and the prediction on the line.

135
00:09:14.440 --> 00:09:18.445
If it's three regressors, an intercept and
two regression variables,

136
00:09:18.445 --> 00:09:22.380
then the fitted line or
the fitted construct

137
00:09:23.830 --> 00:09:29.180
is no longer a line, it's a plane and
the least squares criteria is minimizing

138
00:09:29.180 --> 00:09:32.690
the sum of the squared vertical distances
between the outcomes in the plane.

139
00:09:34.180 --> 00:09:39.465
If we take more than three variables and
intercept and

140
00:09:39.465 --> 00:09:44.040
then let's say three covariants then,
we can't visualize it any more.

141
00:09:44.040 --> 00:09:46.625
So but it's sort of the generalized
idea of the distance,

142
00:09:46.625 --> 00:09:50.129
the vertical distance between the Ys and
some generalized version of a plane.

143
00:09:51.540 --> 00:09:55.460
And what we've seen is the idea that
multi variable regression adjusts for

144
00:09:55.460 --> 00:09:58.140
the other variables in
a sense by taking residuals.

145
00:09:58.140 --> 00:10:02.050
So, for example, if we take this
coefficient in front of X1, beta 1,

146
00:10:02.050 --> 00:10:06.490
the way in which we interpret Beta 1, is
that the remaining regression variables,

147
00:10:06.490 --> 00:10:11.730
X2 up to Xp,
have been linearly removed from both Y and

148
00:10:11.730 --> 00:10:16.200
X1 and then the relationship between those
residuals investigated by themselves.

149
00:10:16.200 --> 00:10:18.120
So in a sense, Y has been adjusted for

150
00:10:18.120 --> 00:10:21.570
the remaining variables, X1 has been
adjusted for the remaining variables, and

151
00:10:21.570 --> 00:10:26.850
we look at those two together so that beta
1 can be thought of as an adjusted effect,

152
00:10:26.850 --> 00:10:30.179
having adjusted for a linear
relationship with the other variables.

153
00:10:31.340 --> 00:10:34.860
So what we're gonna do now is go through
some computer experiments just illustrate

154
00:10:34.860 --> 00:10:38.820
that this is how LM works, and then in
the next lecture we're going to through

155
00:10:38.820 --> 00:10:42.090
some examples and how we interpret
the coefficients in a context.